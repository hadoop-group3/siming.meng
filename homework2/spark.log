[INFO ]20161114@23:40:46,081:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161114@23:40:47,471:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161114@23:40:48,526:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161114@23:40:48,529:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161114@23:40:48,530:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161114@23:40:49,293:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 32774.
[INFO ]20161114@23:40:50,828:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161114@23:40:51,016:Remoting - Starting remoting
[INFO ]20161114@23:40:51,536:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:40629]
[INFO ]20161114@23:40:51,568:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:40629]
[INFO ]20161114@23:40:51,628:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 40629.
[INFO ]20161114@23:40:51,675:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161114@23:40:51,741:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161114@23:40:51,769:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-56cb7711-5a5c-441c-b516-d11ae3b17458
[INFO ]20161114@23:40:51,833:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161114@23:40:52,349:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161114@23:40:53,162:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161114@23:40:53,170:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161114@23:40:53,915:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161114@23:40:53,983:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37105.
[INFO ]20161114@23:40:53,985:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 37105
[INFO ]20161114@23:40:53,987:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161114@23:40:53,995:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:37105 with 500.1 MB RAM, BlockManagerId(driver, localhost, 37105)
[INFO ]20161114@23:40:53,999:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161114@23:41:10,030:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161114@23:41:10,675:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161114@23:41:10,677:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:37105 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161114@23:41:10,696:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:130
[INFO ]20161114@23:41:11,773:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161114@23:41:11,929:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:143
[INFO ]20161114@23:41:11,999:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:143) with 1 output partitions
[INFO ]20161114@23:41:12,002:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:143)
[INFO ]20161114@23:41:12,004:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161114@23:41:12,014:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161114@23:41:12,062:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:130), which has no missing parents
[INFO ]20161114@23:41:12,127:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161114@23:41:12,171:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161114@23:41:12,190:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:37105 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161114@23:41:12,191:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161114@23:41:12,202:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:130)
[INFO ]20161114@23:41:12,206:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161114@23:41:12,337:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161114@23:41:12,369:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161114@23:41:12,432:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161114@23:41:12,435:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161114@23:41:12,472:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161114@23:41:12,473:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161114@23:41:12,473:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161114@23:41:12,473:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161114@23:41:12,473:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161114@23:41:12,644:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161114@23:41:12,644:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:37105 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161114@23:41:12,780:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161114@23:41:12,845:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:143) finished in 0.597 s
[INFO ]20161114@23:41:12,845:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 537 ms on localhost (1/1)
[INFO ]20161114@23:41:12,849:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161114@23:41:12,856:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:143, took 0.919917 s
[INFO ]20161114@23:41:38,803:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161114@23:41:38,854:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161114@23:41:38,885:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161114@23:41:38,889:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161114@23:41:38,892:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161114@23:41:38,905:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161114@23:41:38,929:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161114@23:41:38,943:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161114@23:41:38,955:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161114@23:41:38,956:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-33960e5a-3066-4703-bed2-01330ac80f02
[INFO ]20161114@23:46:23,650:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161114@23:46:24,106:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161114@23:46:24,528:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161114@23:46:24,529:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161114@23:46:24,530:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161114@23:46:24,939:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 47941.
[INFO ]20161114@23:46:25,450:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161114@23:46:25,538:Remoting - Starting remoting
[INFO ]20161114@23:46:25,924:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:42892]
[INFO ]20161114@23:46:25,929:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:42892]
[INFO ]20161114@23:46:25,949:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 42892.
[INFO ]20161114@23:46:25,994:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161114@23:46:26,038:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161114@23:46:26,067:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-212d6868-e4ce-4c67-9a33-7a1261a36866
[INFO ]20161114@23:46:26,101:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161114@23:46:26,222:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161114@23:46:26,953:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161114@23:46:26,955:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161114@23:46:27,236:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161114@23:46:27,310:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32869.
[INFO ]20161114@23:46:27,310:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 32869
[INFO ]20161114@23:46:27,312:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161114@23:46:27,315:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:32869 with 500.1 MB RAM, BlockManagerId(driver, localhost, 32869)
[INFO ]20161114@23:46:27,318:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161114@23:46:28,560:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161114@23:46:29,424:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161114@23:46:29,428:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:32869 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161114@23:46:29,441:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:130
[INFO ]20161114@23:46:30,106:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161114@23:46:30,227:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:143
[INFO ]20161114@23:46:30,293:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:143) with 1 output partitions
[INFO ]20161114@23:46:30,293:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:143)
[INFO ]20161114@23:46:30,295:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161114@23:46:30,299:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161114@23:46:30,318:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:130), which has no missing parents
[INFO ]20161114@23:46:30,364:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161114@23:46:30,421:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161114@23:46:30,421:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:32869 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161114@23:46:30,428:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161114@23:46:30,432:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:130)
[INFO ]20161114@23:46:30,434:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161114@23:46:30,499:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161114@23:46:30,541:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161114@23:46:30,598:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161114@23:46:30,606:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161114@23:46:30,636:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161114@23:46:30,637:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161114@23:46:30,637:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161114@23:46:30,637:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161114@23:46:30,637:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161114@23:46:30,787:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161114@23:46:30,802:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:32869 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161114@23:46:30,914:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161114@23:46:30,999:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 486 ms on localhost (1/1)
[INFO ]20161114@23:46:31,044:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161114@23:46:31,049:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:143) finished in 0.599 s
[INFO ]20161114@23:46:31,079:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:143, took 0.852129 s
[INFO ]20161114@23:46:40,835:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:164
[INFO ]20161114@23:46:40,842:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:164) with 1 output partitions
[INFO ]20161114@23:46:40,842:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:164)
[INFO ]20161114@23:46:40,842:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161114@23:46:40,892:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161114@23:46:40,903:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:159), which has no missing parents
[INFO ]20161114@23:46:40,920:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 343.3 KB)
[INFO ]20161114@23:46:40,936:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 345.3 KB)
[INFO ]20161114@23:46:40,937:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:32869 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161114@23:46:40,941:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161114@23:46:40,941:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:159)
[INFO ]20161114@23:46:40,945:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161114@23:46:40,966:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161114@23:46:40,967:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161114@23:46:40,984:org.apache.spark.CacheManager - Partition rdd_2_0 not found, computing it
[INFO ]20161114@23:46:40,987:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[ERROR]20161115@00:13:22,288:homework2.HW2_Part1 - NumberFormatException:
[INFO ]20161115@00:14:16,668:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:14:17,137:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:14:17,559:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:14:17,559:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:14:17,560:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:14:18,026:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 47619.
[INFO ]20161115@00:14:18,546:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:14:18,632:Remoting - Starting remoting
[INFO ]20161115@00:14:18,989:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:39887]
[INFO ]20161115@00:14:18,991:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:39887]
[INFO ]20161115@00:14:19,002:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 39887.
[INFO ]20161115@00:14:19,069:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:14:19,117:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:14:19,160:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-173f83d5-1ca8-4d50-a486-c973690c7a85
[INFO ]20161115@00:14:19,201:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:14:19,345:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@00:14:20,043:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@00:14:20,064:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@00:14:20,348:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:14:20,488:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46560.
[INFO ]20161115@00:14:20,493:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 46560
[INFO ]20161115@00:14:20,506:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:14:20,513:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:46560 with 500.1 MB RAM, BlockManagerId(driver, localhost, 46560)
[INFO ]20161115@00:14:20,516:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:14:21,992:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:14:22,431:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:14:22,440:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:46560 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:14:22,458:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:157
[INFO ]20161115@00:14:23,169:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:14:23,270:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:170
[INFO ]20161115@00:14:23,319:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:170) with 1 output partitions
[INFO ]20161115@00:14:23,320:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:170)
[INFO ]20161115@00:14:23,321:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:14:23,362:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:14:23,475:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:157), which has no missing parents
[INFO ]20161115@00:14:23,583:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:14:23,671:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:14:23,672:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:46560 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:14:23,673:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:14:23,678:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:157)
[INFO ]20161115@00:14:23,681:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:14:23,782:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:14:23,821:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:14:23,877:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:14:23,881:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:14:23,922:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:14:23,923:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:14:23,923:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:14:23,923:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:14:23,923:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:14:24,069:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:14:24,070:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:46560 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:14:24,188:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:14:24,286:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 515 ms on localhost (1/1)
[INFO ]20161115@00:14:24,386:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:170) finished in 0.588 s
[INFO ]20161115@00:14:24,400:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:14:24,481:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:170, took 1.207653 s
[INFO ]20161115@00:14:27,007:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:191
[INFO ]20161115@00:14:27,010:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:191) with 1 output partitions
[INFO ]20161115@00:14:27,010:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:191)
[INFO ]20161115@00:14:27,010:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:14:27,056:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:14:27,061:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:186), which has no missing parents
[INFO ]20161115@00:14:27,065:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 343.3 KB)
[INFO ]20161115@00:14:27,083:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 345.3 KB)
[INFO ]20161115@00:14:27,083:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:46560 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@00:14:27,086:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:14:27,087:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:186)
[INFO ]20161115@00:14:27,089:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@00:14:27,099:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:14:27,100:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@00:14:27,109:org.apache.spark.CacheManager - Partition rdd_2_0 not found, computing it
[INFO ]20161115@00:14:27,111:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@00:14:27,456:org.apache.spark.storage.MemoryStore - Block rdd_2_0 stored as values in memory (estimated size 510.3 KB, free 855.6 KB)
[INFO ]20161115@00:14:27,461:org.apache.spark.storage.BlockManagerInfo - Added rdd_2_0 in memory on localhost:46560 (size: 510.3 KB, free: 499.4 MB)
[INFO ]20161115@00:14:27,493:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2583 bytes result sent to driver
[INFO ]20161115@00:14:27,528:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:191) finished in 0.419 s
[INFO ]20161115@00:14:27,529:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:191, took 0.520199 s
[INFO ]20161115@00:14:27,530:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 430 ms on localhost (1/1)
[INFO ]20161115@00:14:27,531:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:14:27,537:homework2.HW2_Part1 - Number of records on both the NASDAQ and the SP500:  505
[INFO ]20161115@00:14:29,824:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@00:14:29,884:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@00:14:29,943:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@00:14:29,948:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@00:14:29,997:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@00:14:30,001:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@00:14:30,091:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@00:14:30,118:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@00:14:30,155:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161115@00:14:30,359:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@00:14:30,363:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-a97b3cce-f5ac-479b-a806-b80d2eda6a5c
[INFO ]20161115@00:15:40,893:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:15:41,346:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:15:41,778:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:15:41,779:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:15:41,780:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:15:42,158:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 57500.
[INFO ]20161115@00:15:42,781:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:15:42,875:Remoting - Starting remoting
[INFO ]20161115@00:15:43,246:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:49805]
[INFO ]20161115@00:15:43,251:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:49805]
[INFO ]20161115@00:15:43,271:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 49805.
[INFO ]20161115@00:15:43,326:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:15:43,346:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:15:43,364:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-9631aabf-1a98-4b00-abd0-a1a15352d33e
[INFO ]20161115@00:15:43,404:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:15:43,529:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@00:15:44,194:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@00:15:44,211:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@00:15:44,516:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:15:44,583:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39127.
[INFO ]20161115@00:15:44,584:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 39127
[INFO ]20161115@00:15:44,585:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:15:44,589:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:39127 with 500.1 MB RAM, BlockManagerId(driver, localhost, 39127)
[INFO ]20161115@00:15:44,594:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:15:46,246:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:15:46,781:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:15:46,786:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:39127 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:15:46,856:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:157
[INFO ]20161115@00:15:47,521:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:15:47,692:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:170
[INFO ]20161115@00:15:47,730:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:170) with 1 output partitions
[INFO ]20161115@00:15:47,730:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:170)
[INFO ]20161115@00:15:47,732:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:15:47,736:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:15:47,780:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:157), which has no missing parents
[INFO ]20161115@00:15:47,897:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:15:47,917:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:15:47,917:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:39127 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:15:47,918:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:15:47,922:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:157)
[INFO ]20161115@00:15:47,924:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:15:48,058:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:15:48,110:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:15:48,146:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:15:48,150:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:15:48,170:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:15:48,171:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:15:48,171:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:15:48,171:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:15:48,171:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:15:48,347:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:15:48,359:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:39127 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:15:48,465:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:15:48,561:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:170) finished in 0.613 s
[INFO ]20161115@00:15:48,560:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 542 ms on localhost (1/1)
[INFO ]20161115@00:15:48,565:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:15:48,590:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:170, took 0.891717 s
[INFO ]20161115@00:16:06,587:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:191
[INFO ]20161115@00:16:06,591:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:191) with 1 output partitions
[INFO ]20161115@00:16:06,591:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:191)
[INFO ]20161115@00:16:06,592:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:16:06,598:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:16:06,600:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:186), which has no missing parents
[INFO ]20161115@00:16:06,606:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 343.3 KB)
[INFO ]20161115@00:16:06,634:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 345.3 KB)
[INFO ]20161115@00:16:06,634:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:39127 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@00:16:06,636:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:16:06,636:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:186)
[INFO ]20161115@00:16:06,637:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@00:16:06,651:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:16:06,652:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@00:16:06,669:org.apache.spark.CacheManager - Partition rdd_2_0 not found, computing it
[INFO ]20161115@00:16:06,670:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@00:16:06,842:org.apache.spark.storage.MemoryStore - Block rdd_2_0 stored as values in memory (estimated size 510.3 KB, free 855.6 KB)
[INFO ]20161115@00:16:06,843:org.apache.spark.storage.BlockManagerInfo - Added rdd_2_0 in memory on localhost:39127 (size: 510.3 KB, free: 499.4 MB)
[INFO ]20161115@00:16:06,872:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2583 bytes result sent to driver
[INFO ]20161115@00:16:06,890:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:191) finished in 0.238 s
[INFO ]20161115@00:16:06,892:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:191, took 0.302989 s
[INFO ]20161115@00:16:06,903:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 240 ms on localhost (1/1)
[INFO ]20161115@00:16:06,904:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:16:06,913:homework2.HW2_Part1 - Number of records on both the NASDAQ and the SP500:  505
[INFO ]20161115@00:18:47,000:org.apache.spark.SparkContext - Running Spark version 1.6.0
[INFO ]20161115@00:18:47,003:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:18:47,004:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:18:47,004:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:18:47,015:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 48208.
[INFO ]20161115@00:18:47,074:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:18:47,080:Remoting - Starting remoting
[INFO ]20161115@00:18:47,122:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 55254.
[INFO ]20161115@00:18:47,123:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@localhost:55254]
[INFO ]20161115@00:18:47,123:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@localhost:55254]
[INFO ]20161115@00:18:47,139:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:18:47,139:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:18:47,147:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f6dc05c8-7bb4-4357-b47b-1bbce8dd478b
[INFO ]20161115@00:18:47,148:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:18:47,220:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161115@00:18:47,341:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO ]20161115@00:18:47,405:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
[INFO ]20161115@00:18:47,406:org.apache.spark.ui.SparkUI - Started SparkUI at http://localhost:4041
[INFO ]20161115@00:18:47,585:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:18:47,618:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33873.
[INFO ]20161115@00:18:47,618:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 33873
[INFO ]20161115@00:18:47,619:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:18:47,621:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:33873 with 500.1 MB RAM, BlockManagerId(driver, localhost, 33873)
[INFO ]20161115@00:18:47,622:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:18:58,996:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161115@00:18:59,069:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@00:18:59,463:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@00:18:59,502:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@00:18:59,505:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@00:18:59,507:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@00:18:59,518:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@00:18:59,578:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@00:18:59,579:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161115@00:18:59,698:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://localhost:4041
[INFO ]20161115@00:18:59,708:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@00:18:59,728:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@00:18:59,728:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@00:18:59,729:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@00:18:59,729:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@00:18:59,903:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@00:18:59,916:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@00:18:59,917:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-fdc3ba83-9625-438d-81f3-5981a754428f
[INFO ]20161115@00:18:59,917:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@00:18:59,917:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@00:23:36,333:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:23:37,444:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:23:38,123:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:23:38,124:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:23:38,127:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:23:39,053:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 48097.
[INFO ]20161115@00:23:40,071:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:23:40,265:Remoting - Starting remoting
[INFO ]20161115@00:23:40,810:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:44412]
[INFO ]20161115@00:23:40,815:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:44412]
[INFO ]20161115@00:23:40,915:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 44412.
[INFO ]20161115@00:23:41,018:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:23:41,044:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:23:41,100:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-933f2d8c-e416-4c18-836f-e6caea90ea05
[INFO ]20161115@00:23:41,128:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:23:41,203:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@00:23:41,895:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@00:23:41,903:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@00:23:42,365:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:23:42,435:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56610.
[INFO ]20161115@00:23:42,436:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56610
[INFO ]20161115@00:23:42,438:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:23:42,445:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:56610 with 500.1 MB RAM, BlockManagerId(driver, localhost, 56610)
[INFO ]20161115@00:23:42,448:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:23:44,725:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:23:45,525:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:23:45,529:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56610 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:23:45,543:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:156
[INFO ]20161115@00:23:46,325:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:23:46,531:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:169
[INFO ]20161115@00:23:46,578:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:169) with 1 output partitions
[INFO ]20161115@00:23:46,579:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:169)
[INFO ]20161115@00:23:46,583:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:23:46,589:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:23:46,683:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:156), which has no missing parents
[INFO ]20161115@00:23:46,753:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:23:46,782:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:23:46,783:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56610 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:23:46,784:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:23:46,788:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:156)
[INFO ]20161115@00:23:46,790:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:23:46,975:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:23:47,038:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:23:47,200:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:23:47,204:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:23:47,222:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:23:47,222:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:23:47,223:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:23:47,223:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:23:47,223:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:23:47,358:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:23:47,361:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:56610 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:23:47,483:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:23:47,558:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:169) finished in 0.737 s
[INFO ]20161115@00:23:47,559:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 666 ms on localhost (1/1)
[INFO ]20161115@00:23:47,568:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:23:47,599:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:169, took 1.065366 s
[INFO ]20161115@00:23:57,370:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:190
[INFO ]20161115@00:23:57,375:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:190) with 1 output partitions
[INFO ]20161115@00:23:57,375:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:190)
[INFO ]20161115@00:23:57,375:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:23:57,387:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:23:57,389:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:185), which has no missing parents
[INFO ]20161115@00:23:57,394:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 343.3 KB)
[INFO ]20161115@00:23:57,406:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 345.3 KB)
[INFO ]20161115@00:23:57,406:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:56610 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@00:23:57,407:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:23:57,407:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:185)
[INFO ]20161115@00:23:57,407:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@00:23:57,434:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:23:57,435:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@00:23:57,442:org.apache.spark.CacheManager - Partition rdd_2_0 not found, computing it
[INFO ]20161115@00:23:57,445:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@00:23:57,741:org.apache.spark.storage.MemoryStore - Block rdd_2_0 stored as values in memory (estimated size 510.3 KB, free 855.6 KB)
[INFO ]20161115@00:23:57,748:org.apache.spark.storage.BlockManagerInfo - Added rdd_2_0 in memory on localhost:56610 (size: 510.3 KB, free: 499.4 MB)
[INFO ]20161115@00:23:57,775:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2583 bytes result sent to driver
[INFO ]20161115@00:23:57,789:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:190) finished in 0.378 s
[INFO ]20161115@00:23:57,791:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:190, took 0.416324 s
[INFO ]20161115@00:23:57,794:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 373 ms on localhost (1/1)
[INFO ]20161115@00:23:57,797:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:23:57,805:homework2.HW2_Part1 - --->Number of records on both the NASDAQ and the SP500:  505
[INFO ]20161115@00:27:38,973:org.apache.spark.SparkContext - Running Spark version 1.6.0
[INFO ]20161115@00:27:38,983:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:27:38,983:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:27:38,983:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:27:39,078:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56306.
[INFO ]20161115@00:27:39,125:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:27:39,131:Remoting - Starting remoting
[INFO ]20161115@00:27:39,178:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 60213.
[INFO ]20161115@00:27:39,178:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@localhost:60213]
[INFO ]20161115@00:27:39,182:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@localhost:60213]
[INFO ]20161115@00:27:39,189:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:27:39,214:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:27:39,223:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-57835326-f091-40cb-ae2a-37c15c9038cd
[INFO ]20161115@00:27:39,223:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:27:39,232:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161115@00:27:39,465:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO ]20161115@00:27:39,509:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
[INFO ]20161115@00:27:39,509:org.apache.spark.ui.SparkUI - Started SparkUI at http://localhost:4041
[INFO ]20161115@00:27:39,624:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:27:39,640:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36213.
[INFO ]20161115@00:27:39,641:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 36213
[INFO ]20161115@00:27:39,641:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:27:39,641:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:36213 with 500.1 MB RAM, BlockManagerId(driver, localhost, 36213)
[INFO ]20161115@00:27:39,641:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:27:45,236:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161115@00:27:45,402:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@00:27:45,472:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@00:27:45,536:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@00:27:45,540:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@00:27:45,543:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@00:27:45,552:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@00:27:45,590:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@00:27:45,591:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161115@00:27:45,632:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@00:27:45,647:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161115@00:27:45,678:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://localhost:4041
[INFO ]20161115@00:27:45,683:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@00:27:45,699:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@00:27:45,699:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@00:27:45,702:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@00:27:45,703:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@00:27:45,703:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161115@00:27:45,704:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@00:27:45,746:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@00:27:45,747:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@00:27:45,748:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-6ac4a47f-f8fd-4086-8e6b-211ceae45db3
[INFO ]20161115@00:28:33,934:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:28:34,381:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:28:34,792:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:28:34,793:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:28:34,793:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:28:35,154:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 43495.
[INFO ]20161115@00:28:35,645:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:28:35,734:Remoting - Starting remoting
[INFO ]20161115@00:28:36,137:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:39196]
[INFO ]20161115@00:28:36,138:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:39196]
[INFO ]20161115@00:28:36,168:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 39196.
[INFO ]20161115@00:28:36,239:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:28:36,298:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:28:36,346:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-2d3b5f48-dafd-4690-a727-0648110e2e2f
[INFO ]20161115@00:28:36,390:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:28:36,544:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@00:28:37,158:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@00:28:37,181:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@00:28:37,437:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:28:37,553:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55794.
[INFO ]20161115@00:28:37,554:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 55794
[INFO ]20161115@00:28:37,555:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:28:37,569:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:55794 with 500.1 MB RAM, BlockManagerId(driver, localhost, 55794)
[INFO ]20161115@00:28:37,577:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:28:39,075:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:28:39,547:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:28:39,555:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:55794 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:28:39,621:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:156
[INFO ]20161115@00:28:40,453:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:28:40,714:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:169
[INFO ]20161115@00:28:40,783:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:169) with 1 output partitions
[INFO ]20161115@00:28:40,791:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:169)
[INFO ]20161115@00:28:40,803:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:28:40,808:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:28:40,838:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:156), which has no missing parents
[INFO ]20161115@00:28:40,879:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:28:40,928:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:28:40,930:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:55794 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:28:40,931:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:28:40,935:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:156)
[INFO ]20161115@00:28:40,937:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:28:41,028:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:28:41,100:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:28:41,220:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:28:41,235:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:28:41,267:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:28:41,268:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:28:41,268:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:28:41,268:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:28:41,268:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:28:41,414:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:28:41,427:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:55794 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:28:41,535:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:28:41,613:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 591 ms on localhost (1/1)
[INFO ]20161115@00:28:41,631:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:28:41,638:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:169) finished in 0.679 s
[INFO ]20161115@00:28:41,656:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:169, took 0.935994 s
[INFO ]20161115@00:29:02,480:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:190
[INFO ]20161115@00:29:02,482:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:190) with 1 output partitions
[INFO ]20161115@00:29:02,482:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:190)
[INFO ]20161115@00:29:02,483:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:29:02,488:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:29:02,489:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:185), which has no missing parents
[INFO ]20161115@00:29:02,497:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 343.3 KB)
[INFO ]20161115@00:29:02,511:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 345.3 KB)
[INFO ]20161115@00:29:02,512:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:55794 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@00:29:02,513:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:29:02,514:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:185)
[INFO ]20161115@00:29:02,515:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@00:29:02,523:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:29:02,534:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@00:29:02,541:org.apache.spark.CacheManager - Partition rdd_2_0 not found, computing it
[INFO ]20161115@00:29:02,551:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@00:29:02,728:org.apache.spark.storage.MemoryStore - Block rdd_2_0 stored as values in memory (estimated size 510.3 KB, free 855.6 KB)
[INFO ]20161115@00:29:02,731:org.apache.spark.storage.BlockManagerInfo - Added rdd_2_0 in memory on localhost:55794 (size: 510.3 KB, free: 499.4 MB)
[INFO ]20161115@00:29:02,757:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2583 bytes result sent to driver
[INFO ]20161115@00:29:02,776:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:190) finished in 0.251 s
[INFO ]20161115@00:29:02,778:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:190, took 0.296016 s
[INFO ]20161115@00:29:02,791:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 254 ms on localhost (1/1)
[INFO ]20161115@00:29:02,792:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:29:02,793:homework2.HW2_Part1 - --->Number of valid records for SP500:  505
[INFO ]20161115@00:32:29,542:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:32:30,490:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:32:31,166:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:32:31,167:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:32:31,169:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:32:31,991:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 48231.
[INFO ]20161115@00:32:32,722:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:32:33,133:Remoting - Starting remoting
[INFO ]20161115@00:32:33,609:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:39826]
[INFO ]20161115@00:32:33,610:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:39826]
[INFO ]20161115@00:32:33,637:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 39826.
[INFO ]20161115@00:32:33,683:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:32:33,727:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:32:33,789:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-3b268e2e-7bab-49e2-9092-323ff9fe80f1
[INFO ]20161115@00:32:33,806:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:32:33,917:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161115@00:32:34,692:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[INFO ]20161115@00:32:34,737:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
[INFO ]20161115@00:32:34,741:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4041
[INFO ]20161115@00:32:35,069:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:32:35,114:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42806.
[INFO ]20161115@00:32:35,123:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 42806
[INFO ]20161115@00:32:35,178:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:32:35,232:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:42806 with 500.1 MB RAM, BlockManagerId(driver, localhost, 42806)
[INFO ]20161115@00:32:35,234:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:32:36,453:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:32:37,048:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:32:37,055:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:42806 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:32:37,070:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:156
[INFO ]20161115@00:32:37,721:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:32:37,878:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:169
[INFO ]20161115@00:32:38,004:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:169) with 1 output partitions
[INFO ]20161115@00:32:38,006:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:169)
[INFO ]20161115@00:32:38,010:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:32:38,028:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:32:38,142:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:156), which has no missing parents
[INFO ]20161115@00:32:38,352:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:32:38,387:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:32:38,388:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:42806 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:32:38,389:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:32:38,395:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:156)
[INFO ]20161115@00:32:38,397:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:32:38,516:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:32:38,567:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:32:38,655:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:32:38,662:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:32:38,701:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:32:38,701:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:32:38,701:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:32:38,702:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:32:38,702:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:32:38,914:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:32:38,915:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:42806 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:32:39,040:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:32:39,128:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 590 ms on localhost (1/1)
[INFO ]20161115@00:32:39,220:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:169) finished in 0.735 s
[INFO ]20161115@00:32:39,237:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:32:39,254:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:169, took 1.374905 s
[INFO ]20161115@00:32:48,302:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@00:32:48,366:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@00:32:48,409:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@00:32:48,413:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@00:32:48,415:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@00:32:48,418:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@00:32:48,436:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@00:32:48,443:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@00:32:48,453:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161115@00:32:48,570:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@00:32:48,571:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-241d4eb9-4eab-44ac-80a9-212d02218428
[INFO ]20161115@00:33:21,027:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:33:21,577:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:33:21,993:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:33:21,994:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:33:21,994:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:33:22,469:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49176.
[INFO ]20161115@00:33:23,264:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:33:23,333:Remoting - Starting remoting
[INFO ]20161115@00:33:23,662:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:51055]
[INFO ]20161115@00:33:23,665:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:51055]
[INFO ]20161115@00:33:23,682:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 51055.
[INFO ]20161115@00:33:23,743:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:33:23,791:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:33:23,828:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-0aad79bf-3068-4f51-b7f2-01b7435662d4
[INFO ]20161115@00:33:23,874:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:33:23,996:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@00:33:24,561:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@00:33:24,572:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@00:33:24,920:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:33:25,011:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52700.
[INFO ]20161115@00:33:25,017:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 52700
[INFO ]20161115@00:33:25,018:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:33:25,020:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:52700 with 500.1 MB RAM, BlockManagerId(driver, localhost, 52700)
[INFO ]20161115@00:33:25,024:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:33:26,645:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:33:27,053:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:33:27,057:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:52700 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:33:27,093:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:156
[INFO ]20161115@00:33:27,682:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:33:27,877:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:169
[INFO ]20161115@00:33:27,932:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:169) with 1 output partitions
[INFO ]20161115@00:33:27,933:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:169)
[INFO ]20161115@00:33:27,943:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:33:27,949:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:33:27,971:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:156), which has no missing parents
[INFO ]20161115@00:33:28,053:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:33:28,087:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:33:28,088:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:52700 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:33:28,091:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:33:28,096:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:156)
[INFO ]20161115@00:33:28,100:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:33:28,184:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:33:28,204:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:33:28,257:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:33:28,266:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:33:28,312:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:33:28,316:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:33:28,316:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:33:28,316:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:33:28,317:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:33:28,523:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:33:28,524:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:52700 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:33:28,636:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:33:28,719:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 558 ms on localhost (1/1)
[INFO ]20161115@00:33:28,734:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:169) finished in 0.610 s
[INFO ]20161115@00:33:28,735:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:33:28,776:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:169, took 0.897336 s
[INFO ]20161115@00:41:16,153:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:41:16,730:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:41:17,405:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:41:17,407:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:41:17,409:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:41:17,987:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45708.
[INFO ]20161115@00:41:18,693:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:41:18,918:Remoting - Starting remoting
[INFO ]20161115@00:41:19,683:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:34482]
[INFO ]20161115@00:41:19,711:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:34482]
[INFO ]20161115@00:41:19,786:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 34482.
[INFO ]20161115@00:41:19,832:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:41:19,981:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:41:20,001:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-d50ca10b-be91-4e8b-8a54-8b8623626d7c
[INFO ]20161115@00:41:20,018:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:41:20,241:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@00:41:21,023:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@00:41:21,057:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@00:41:21,340:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:41:21,376:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33955.
[INFO ]20161115@00:41:21,378:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 33955
[INFO ]20161115@00:41:21,380:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:41:21,383:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:33955 with 500.1 MB RAM, BlockManagerId(driver, localhost, 33955)
[INFO ]20161115@00:41:21,408:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:41:24,165:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:41:24,751:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:41:24,756:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:41:24,776:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:160
[INFO ]20161115@00:41:25,353:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:41:25,501:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:173
[INFO ]20161115@00:41:25,557:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:173) with 1 output partitions
[INFO ]20161115@00:41:25,557:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:173)
[INFO ]20161115@00:41:25,561:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:41:25,569:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:41:25,598:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160), which has no missing parents
[INFO ]20161115@00:41:25,684:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:41:25,726:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:41:25,731:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:41:25,733:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:41:25,774:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160)
[INFO ]20161115@00:41:25,780:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:41:25,928:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:41:25,976:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:41:26,028:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:41:26,043:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:41:26,141:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:41:26,142:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:41:26,142:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:41:26,142:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:41:26,142:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:41:26,289:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:41:26,295:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:41:26,405:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:41:26,477:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 567 ms on localhost (1/1)
[INFO ]20161115@00:41:26,490:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:173) finished in 0.640 s
[INFO ]20161115@00:41:26,495:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:41:26,506:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:173, took 0.998186 s
[INFO ]20161115@00:41:37,225:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:194
[INFO ]20161115@00:41:37,229:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:194) with 1 output partitions
[INFO ]20161115@00:41:37,230:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:194)
[INFO ]20161115@00:41:37,230:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:41:37,238:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:41:37,240:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:189), which has no missing parents
[INFO ]20161115@00:41:37,242:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 343.3 KB)
[INFO ]20161115@00:41:37,250:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 345.3 KB)
[INFO ]20161115@00:41:37,251:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@00:41:37,252:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:41:37,252:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:189)
[INFO ]20161115@00:41:37,252:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@00:41:37,257:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:41:37,257:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@00:41:37,260:org.apache.spark.CacheManager - Partition rdd_2_0 not found, computing it
[INFO ]20161115@00:41:37,261:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@00:42:25,853:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:42:26,710:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:42:27,315:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:42:27,316:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:42:27,317:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:42:27,969:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35351.
[INFO ]20161115@00:42:28,714:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:42:28,988:Remoting - Starting remoting
[INFO ]20161115@00:42:29,540:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:59348]
[INFO ]20161115@00:42:29,547:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:59348]
[INFO ]20161115@00:42:29,581:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 59348.
[INFO ]20161115@00:42:29,657:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:42:29,904:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:42:29,929:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-6c27e51e-5b17-4b58-922e-a294dbf569bf
[INFO ]20161115@00:42:29,997:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:42:30,059:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161115@00:42:31,151:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN ]20161115@00:42:31,248:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO ]20161115@00:42:31,305:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4042.
[INFO ]20161115@00:42:31,315:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4042
[INFO ]20161115@00:42:31,596:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:42:31,658:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39448.
[INFO ]20161115@00:42:31,666:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 39448
[INFO ]20161115@00:42:31,668:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:42:31,681:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:39448 with 500.1 MB RAM, BlockManagerId(driver, localhost, 39448)
[INFO ]20161115@00:42:31,691:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:42:34,349:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:42:35,111:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:42:35,115:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:39448 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:42:35,145:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:160
[INFO ]20161115@00:42:36,188:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:42:36,367:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:173
[INFO ]20161115@00:42:36,418:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:173) with 1 output partitions
[INFO ]20161115@00:42:36,419:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:173)
[INFO ]20161115@00:42:36,420:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:42:36,427:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:42:36,448:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160), which has no missing parents
[INFO ]20161115@00:42:36,532:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:42:36,573:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:42:36,574:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:39448 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:42:36,575:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:42:36,579:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160)
[INFO ]20161115@00:42:36,581:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:42:36,653:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:42:36,734:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:42:36,795:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:42:36,803:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:42:36,875:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:42:36,876:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:42:36,876:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:42:36,876:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:42:36,876:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:42:37,024:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:42:37,025:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:39448 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:42:37,161:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:42:37,247:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 598 ms on localhost (1/1)
[INFO ]20161115@00:42:37,251:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:173) finished in 0.644 s
[INFO ]20161115@00:42:37,262:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:42:37,277:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:173, took 0.909577 s
[INFO ]20161115@00:42:43,491:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:194
[INFO ]20161115@00:42:43,493:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:194) with 1 output partitions
[INFO ]20161115@00:42:43,493:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:194)
[INFO ]20161115@00:42:43,494:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:42:43,511:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:42:43,513:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:189), which has no missing parents
[INFO ]20161115@00:42:43,514:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 343.3 KB)
[INFO ]20161115@00:42:43,521:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 345.3 KB)
[INFO ]20161115@00:42:43,526:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:39448 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@00:42:43,526:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:42:43,527:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:189)
[INFO ]20161115@00:42:43,527:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@00:42:43,607:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:42:43,608:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@00:42:43,653:org.apache.spark.CacheManager - Partition rdd_2_0 not found, computing it
[INFO ]20161115@00:42:43,654:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@00:43:08,904:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:43:09,848:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:43:10,726:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:43:10,727:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:43:10,728:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:43:11,376:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 33460.
[INFO ]20161115@00:43:12,163:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:43:12,278:Remoting - Starting remoting
[INFO ]20161115@00:43:12,910:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:37118]
[INFO ]20161115@00:43:12,913:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:37118]
[INFO ]20161115@00:43:12,930:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 37118.
[INFO ]20161115@00:43:13,018:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:43:13,051:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:43:13,070:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-8ae521a6-be9e-4981-942a-d65b949c4d22
[INFO ]20161115@00:43:13,086:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:43:13,225:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161115@00:43:13,997:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN ]20161115@00:43:14,092:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN ]20161115@00:43:14,198:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[INFO ]20161115@00:43:14,278:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4043.
[INFO ]20161115@00:43:14,289:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4043
[INFO ]20161115@00:43:14,624:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:43:14,647:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47612.
[INFO ]20161115@00:43:14,648:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 47612
[INFO ]20161115@00:43:14,650:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:43:14,738:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:47612 with 500.1 MB RAM, BlockManagerId(driver, localhost, 47612)
[INFO ]20161115@00:43:14,749:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:43:17,074:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:43:18,018:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:43:18,024:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:47612 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:43:18,040:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:160
[INFO ]20161115@00:43:19,131:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:43:19,283:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:173
[INFO ]20161115@00:43:19,327:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:173) with 1 output partitions
[INFO ]20161115@00:43:19,328:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:173)
[INFO ]20161115@00:43:19,328:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:43:19,340:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:43:19,379:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160), which has no missing parents
[INFO ]20161115@00:43:19,531:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:43:19,561:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:43:19,561:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:47612 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:43:19,563:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:43:19,567:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160)
[INFO ]20161115@00:43:19,569:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:43:19,638:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:43:19,715:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:43:19,798:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:43:19,804:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:43:19,829:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:43:19,829:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:43:19,829:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:43:19,830:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:43:19,830:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:43:20,009:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:43:20,009:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:47612 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:43:20,108:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:43:20,191:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 562 ms on localhost (1/1)
[INFO ]20161115@00:43:20,210:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:173) finished in 0.606 s
[INFO ]20161115@00:43:20,212:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:43:20,224:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:173, took 0.940087 s
[INFO ]20161115@00:44:53,308:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:44:54,155:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:44:54,898:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:44:54,899:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:44:54,900:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:44:55,400:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 52764.
[INFO ]20161115@00:44:56,446:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:44:56,649:Remoting - Starting remoting
[INFO ]20161115@00:44:57,192:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:52984]
[INFO ]20161115@00:44:57,201:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:52984]
[INFO ]20161115@00:44:57,239:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 52984.
[INFO ]20161115@00:44:57,290:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:44:57,427:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:44:57,447:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-1248bfdc-2183-44df-8f05-8c12114e6d67
[INFO ]20161115@00:44:57,470:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:44:58,106:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161115@00:44:59,512:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN ]20161115@00:44:59,604:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[WARN ]20161115@00:44:59,705:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[WARN ]20161115@00:44:59,793:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[INFO ]20161115@00:44:59,827:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4044.
[INFO ]20161115@00:44:59,835:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4044
[INFO ]20161115@00:45:00,440:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:45:00,514:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50144.
[INFO ]20161115@00:45:00,599:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 50144
[INFO ]20161115@00:45:00,601:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:45:00,633:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:50144 with 500.1 MB RAM, BlockManagerId(driver, localhost, 50144)
[INFO ]20161115@00:45:00,639:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:45:02,123:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:45:02,597:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:45:02,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:50144 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:45:02,639:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:160
[INFO ]20161115@00:45:03,395:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:45:03,628:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:173
[INFO ]20161115@00:45:03,683:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:173) with 1 output partitions
[INFO ]20161115@00:45:03,683:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:173)
[INFO ]20161115@00:45:03,684:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:45:03,688:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:45:03,737:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160), which has no missing parents
[INFO ]20161115@00:45:03,795:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:45:03,819:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:45:03,819:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:50144 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:45:03,821:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:45:03,829:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160)
[INFO ]20161115@00:45:03,832:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:45:03,913:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:45:03,942:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:45:03,988:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:45:03,993:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:45:04,069:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:45:04,069:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:45:04,070:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:45:04,070:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:45:04,070:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:45:04,207:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:45:04,212:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:50144 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:45:04,323:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:45:04,446:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 547 ms on localhost (1/1)
[INFO ]20161115@00:45:04,466:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:173) finished in 0.607 s
[INFO ]20161115@00:45:04,465:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:45:04,489:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:173, took 0.861091 s
[INFO ]20161115@00:45:16,432:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:45:16,932:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:45:17,358:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:45:17,359:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:45:17,360:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:45:17,765:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 40962.
[INFO ]20161115@00:45:18,344:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:45:18,419:Remoting - Starting remoting
[INFO ]20161115@00:45:18,769:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:51599]
[INFO ]20161115@00:45:18,771:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:51599]
[INFO ]20161115@00:45:18,782:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 51599.
[INFO ]20161115@00:45:18,857:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:45:18,900:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:45:18,933:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f9e8f9f9-6f62-4593-8b02-10fe7a045be5
[INFO ]20161115@00:45:18,970:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:45:19,092:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161115@00:45:19,868:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN ]20161115@00:45:19,961:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO ]20161115@00:45:20,009:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4042.
[INFO ]20161115@00:45:20,034:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4042
[INFO ]20161115@00:45:20,301:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:45:20,400:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38493.
[INFO ]20161115@00:45:20,401:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 38493
[INFO ]20161115@00:45:20,402:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:45:20,404:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:38493 with 500.1 MB RAM, BlockManagerId(driver, localhost, 38493)
[INFO ]20161115@00:45:20,406:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:45:21,888:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:45:22,397:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:45:22,405:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:38493 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:45:22,418:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:160
[INFO ]20161115@00:45:23,533:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:45:23,683:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:173
[INFO ]20161115@00:45:23,943:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:173) with 1 output partitions
[INFO ]20161115@00:45:23,944:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:173)
[INFO ]20161115@00:45:23,951:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:45:23,971:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:45:24,134:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160), which has no missing parents
[INFO ]20161115@00:45:24,179:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:45:24,242:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:45:24,254:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:38493 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:45:24,255:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:45:24,269:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160)
[INFO ]20161115@00:45:24,281:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:45:24,348:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:45:24,367:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:45:24,433:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:45:24,438:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:45:24,455:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:45:24,455:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:45:24,455:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:45:24,455:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:45:24,455:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:45:24,632:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:45:24,638:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:38493 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:45:24,787:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:45:24,889:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 525 ms on localhost (1/1)
[INFO ]20161115@00:45:24,901:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:45:24,915:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:173) finished in 0.612 s
[INFO ]20161115@00:45:24,937:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:173, took 1.252223 s
[INFO ]20161115@00:45:33,934:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:194
[INFO ]20161115@00:45:33,936:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:194) with 1 output partitions
[INFO ]20161115@00:45:33,936:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:194)
[INFO ]20161115@00:45:33,936:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:45:33,942:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:45:33,946:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:189), which has no missing parents
[INFO ]20161115@00:45:33,954:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 343.3 KB)
[INFO ]20161115@00:45:33,968:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 345.3 KB)
[INFO ]20161115@00:45:33,969:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:38493 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@00:45:33,970:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:45:33,971:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:189)
[INFO ]20161115@00:45:33,972:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@00:45:34,019:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:45:34,022:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@00:45:34,027:org.apache.spark.CacheManager - Partition rdd_2_0 not found, computing it
[INFO ]20161115@00:45:34,029:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@00:46:35,445:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@00:46:35,932:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@00:46:36,371:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@00:46:36,373:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@00:46:36,375:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@00:46:36,839:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44504.
[INFO ]20161115@00:46:37,379:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@00:46:37,459:Remoting - Starting remoting
[INFO ]20161115@00:46:37,819:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:36280]
[INFO ]20161115@00:46:37,822:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:36280]
[INFO ]20161115@00:46:37,849:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 36280.
[INFO ]20161115@00:46:37,903:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@00:46:37,945:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@00:46:37,976:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-6f4586eb-2665-4c34-abc7-974242788df6
[INFO ]20161115@00:46:38,017:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@00:46:38,139:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[WARN ]20161115@00:46:38,812:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[WARN ]20161115@00:46:38,910:org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[INFO ]20161115@00:46:38,975:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4042.
[INFO ]20161115@00:46:38,992:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4042
[INFO ]20161115@00:46:39,259:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@00:46:39,347:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58145.
[INFO ]20161115@00:46:39,347:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 58145
[INFO ]20161115@00:46:39,349:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@00:46:39,352:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:58145 with 500.1 MB RAM, BlockManagerId(driver, localhost, 58145)
[INFO ]20161115@00:46:39,354:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@00:46:41,280:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@00:46:41,810:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@00:46:41,813:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:58145 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@00:46:41,826:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:160
[INFO ]20161115@00:46:42,689:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@00:46:42,801:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:173
[INFO ]20161115@00:46:42,944:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:173) with 1 output partitions
[INFO ]20161115@00:46:42,946:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:173)
[INFO ]20161115@00:46:42,967:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:46:42,986:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:46:43,092:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160), which has no missing parents
[INFO ]20161115@00:46:43,168:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@00:46:43,228:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@00:46:43,230:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:58145 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@00:46:43,231:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:46:43,236:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:160)
[INFO ]20161115@00:46:43,238:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@00:46:43,370:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:46:43,397:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@00:46:43,494:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@00:46:43,502:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@00:46:43,586:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@00:46:43,587:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@00:46:43,587:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@00:46:43,587:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@00:46:43,587:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@00:46:43,803:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@00:46:43,812:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:58145 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@00:46:43,967:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@00:46:44,225:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 782 ms on localhost (1/1)
[INFO ]20161115@00:46:44,314:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:46:44,327:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:173) finished in 1.069 s
[INFO ]20161115@00:46:44,389:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:173, took 1.586601 s
[INFO ]20161115@00:47:21,757:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:194
[INFO ]20161115@00:47:21,760:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:194) with 1 output partitions
[INFO ]20161115@00:47:21,760:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:194)
[INFO ]20161115@00:47:21,760:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@00:47:21,772:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@00:47:21,775:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:189), which has no missing parents
[INFO ]20161115@00:47:21,780:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 343.3 KB)
[INFO ]20161115@00:47:21,796:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1980.0 B, free 345.3 KB)
[INFO ]20161115@00:47:21,797:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:58145 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@00:47:21,798:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@00:47:21,798:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mapToPair at HW2_Part1.java:189)
[INFO ]20161115@00:47:21,799:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@00:47:21,806:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@00:47:21,806:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@00:47:21,811:org.apache.spark.CacheManager - Partition rdd_2_0 not found, computing it
[INFO ]20161115@00:47:21,813:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@00:48:13,549:org.apache.spark.storage.MemoryStore - Block rdd_2_0 stored as values in memory (estimated size 510.3 KB, free 855.6 KB)
[INFO ]20161115@00:48:13,561:org.apache.spark.storage.BlockManagerInfo - Added rdd_2_0 in memory on localhost:58145 (size: 510.3 KB, free: 499.4 MB)
[INFO ]20161115@00:48:13,622:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2583 bytes result sent to driver
[INFO ]20161115@00:48:13,679:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:194) finished in 51.874 s
[INFO ]20161115@00:48:13,680:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:194, took 51.921288 s
[INFO ]20161115@00:48:13,682:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 51874 ms on localhost (1/1)
[INFO ]20161115@00:48:13,685:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@00:48:13,693:homework2.HW2_Part1 - --->Number of valid records for SP500:  505
[INFO ]20161115@00:48:16,925:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4042
[INFO ]20161115@00:48:16,996:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@00:48:17,099:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@00:48:17,104:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@00:48:17,106:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@00:48:17,110:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@00:48:17,140:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@00:48:17,188:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@00:48:17,193:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161115@00:48:17,262:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@00:48:17,264:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-68879e11-3815-4433-85dd-e984bb05f089
[WARN ]20161115@20:17:29,041:org.apache.spark.HeartbeatReceiver - Removing executor driver with no recent heartbeats: 70093322 ms exceeds timeout 120000 ms
[ERROR]20161115@20:17:29,052:org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor driver on localhost: Executor heartbeat timed out after 70093322 ms
[WARN ]20161115@20:17:29,061:org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 1.0 (TID 1, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 70093322 ms
[ERROR]20161115@20:17:29,099:org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
[INFO ]20161115@20:17:29,111:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@20:17:29,119:org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 1
[INFO ]20161115@20:17:29,121:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:194) failed in 70551.865 s
[INFO ]20161115@20:17:29,133:org.apache.spark.scheduler.DAGScheduler - Job 1 failed: count at HW2_Part1.java:194, took 464.027229 s
[INFO ]20161115@20:17:29,184:org.apache.spark.scheduler.DAGScheduler - Executor lost: driver (epoch 0)
[INFO ]20161115@20:17:29,200:org.apache.spark.storage.BlockManagerMasterEndpoint - Trying to remove executor driver from BlockManagerMaster.
[WARN ]20161115@20:17:29,203:org.apache.spark.SparkContext - Killing executors is only supported in coarse-grained mode
[INFO ]20161115@20:17:29,205:org.apache.spark.storage.BlockManagerMasterEndpoint - Removing block manager BlockManagerId(driver, localhost, 33955)
[INFO ]20161115@20:17:29,206:org.apache.spark.storage.BlockManagerMaster - Removed driver successfully in removeExecutor
[INFO ]20161115@20:17:29,215:org.apache.spark.scheduler.DAGScheduler - Host added was in lost list earlier: localhost
[INFO ]20161115@20:17:33,589:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:17:33,593:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:17:33,593:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:17:33,594:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:33955 with 500.1 MB RAM, BlockManagerId(driver, localhost, 33955)
[INFO ]20161115@20:17:33,595:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:17:33,601:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:17:33,605:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 500.1 MB)
[INFO ]20161115@20:17:33,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@20:17:33,613:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@20:17:33,614:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:17:43,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:17:43,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:17:43,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:17:43,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:17:43,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:17:43,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:17:43,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:17:43,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:17:43,588:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:17:53,963:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:17:54,058:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:17:54,058:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:17:54,059:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:17:54,059:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:17:54,059:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:17:54,060:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:17:54,060:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:17:54,092:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:18:03,727:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:18:03,736:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:18:03,744:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:18:03,746:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:18:03,746:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:18:03,896:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:03,901:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:18:03,906:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:03,908:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:18:13,589:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:18:13,610:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:18:13,612:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:18:13,613:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:18:13,613:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:18:13,614:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:13,616:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:18:13,617:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:13,617:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:18:23,590:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:18:23,591:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:18:23,591:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:18:23,593:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:18:23,593:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:18:23,594:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:23,594:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:18:23,596:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:23,596:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:18:33,595:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:18:33,597:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:18:33,597:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:18:33,597:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:18:33,598:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:18:33,615:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:33,623:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:18:33,625:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:33,626:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:18:43,603:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:18:43,605:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:18:43,605:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:18:43,605:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:18:43,605:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:18:43,606:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:43,606:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:18:43,606:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:43,607:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:18:53,591:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:18:53,595:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:18:53,596:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:18:53,598:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:18:53,603:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:18:53,604:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:53,609:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:18:53,611:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:18:53,612:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:19:03,589:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:19:03,623:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:19:03,624:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:19:03,625:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:19:03,626:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:19:03,627:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:03,629:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:19:03,633:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:03,635:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:19:13,603:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:19:13,605:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:19:13,605:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:19:13,606:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:19:13,606:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:19:13,606:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:13,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:19:13,608:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:13,648:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:19:23,647:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:19:23,666:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:19:23,666:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:19:23,667:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:19:23,667:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:19:23,685:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:23,688:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:19:23,689:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:23,691:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:19:33,594:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:19:33,595:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:19:33,595:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:19:33,596:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:19:33,596:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:19:33,596:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:33,598:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:19:33,599:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:33,599:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:19:43,621:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:19:43,632:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:19:43,632:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:19:43,633:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:19:43,633:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:19:43,641:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:43,642:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:19:43,642:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:43,642:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:19:53,600:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:19:53,601:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:19:53,601:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:19:53,601:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:19:53,601:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:19:53,610:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:53,635:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:19:53,636:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:19:53,653:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:20:03,653:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:20:03,659:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:20:03,660:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:20:03,661:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:20:03,661:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:20:03,663:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:03,667:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:20:03,733:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:03,736:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:20:13,587:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:20:13,589:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:20:13,590:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:20:13,593:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:20:13,593:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:20:13,604:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:13,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:20:13,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:13,608:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:20:23,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:20:23,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:20:23,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:20:23,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:20:23,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:20:23,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:23,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:20:23,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:23,588:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:20:33,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:20:33,586:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:20:33,587:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:20:33,587:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:20:33,587:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:20:33,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:33,590:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:20:33,592:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:33,593:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:20:43,584:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:20:43,586:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:20:43,586:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:20:43,587:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:20:43,587:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:20:43,596:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:43,597:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:20:43,597:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:43,598:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:20:53,588:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:20:53,589:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:20:53,589:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:20:53,589:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:20:53,589:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:20:53,601:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:53,606:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:20:53,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:20:53,608:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:21:03,604:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:21:03,623:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:21:03,623:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:21:03,624:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:21:03,624:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:21:03,625:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:03,625:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:21:03,626:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:03,626:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:21:13,602:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:21:13,613:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:21:13,613:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:21:13,613:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:21:13,613:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:21:13,629:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:13,634:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:21:13,636:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:13,637:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:21:23,617:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:21:23,638:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:21:23,638:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:21:23,638:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:21:23,639:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:21:23,640:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:23,641:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:21:23,646:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:23,648:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:21:33,601:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:21:33,602:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:21:33,602:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:21:33,602:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:21:33,602:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:21:33,604:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:33,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:21:33,609:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:33,610:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:21:43,613:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:21:43,615:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:21:43,615:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:21:43,615:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:21:43,615:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:21:43,675:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:43,677:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:21:43,679:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:43,680:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:21:53,591:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:21:53,614:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:21:53,614:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:21:53,614:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:21:53,614:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:21:53,615:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:53,638:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:21:53,640:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:21:53,640:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:22:03,600:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:22:03,601:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:22:03,601:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:22:03,601:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:22:03,601:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:22:03,602:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:03,627:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:22:03,627:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:03,648:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:22:13,594:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:22:13,595:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:22:13,595:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:22:13,608:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:22:13,608:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:22:13,609:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:13,609:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:22:13,610:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:13,610:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:22:23,615:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:22:23,617:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:22:23,617:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:22:23,626:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:22:23,626:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:22:23,626:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:23,627:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:22:23,731:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:23,733:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:22:33,587:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:22:33,605:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:22:33,605:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:22:33,606:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:22:33,606:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:22:33,606:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:33,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:22:33,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:33,607:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:22:43,595:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:22:43,596:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:22:43,596:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:22:43,596:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:22:43,597:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:22:43,598:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:43,598:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:22:43,598:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:43,598:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:22:53,691:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:22:53,730:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:22:53,730:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:22:53,733:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:22:53,733:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:22:53,735:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:53,736:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:22:53,737:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:22:53,747:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:23:03,599:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:23:03,606:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:23:03,606:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:23:03,606:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:23:03,606:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:23:03,606:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:03,606:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:23:03,607:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:03,607:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:23:13,602:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:23:13,605:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:23:13,605:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:23:13,605:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:23:13,608:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:23:13,610:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:13,612:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:23:13,613:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:13,616:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:23:23,614:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:23:23,616:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:23:23,616:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:23:23,617:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:23:23,617:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:23:23,618:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:23,618:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:23:23,644:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:23,646:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:23:33,609:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:23:33,618:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:23:33,618:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:23:33,618:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:23:33,618:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:23:33,619:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:33,640:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:23:33,644:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:33,646:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:23:43,613:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:23:43,614:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:23:43,633:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:23:43,634:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:23:43,634:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:23:43,635:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:43,636:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:23:43,637:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:43,637:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:23:53,637:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:23:53,639:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:23:53,639:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:23:53,639:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:23:53,639:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:23:53,640:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:53,659:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:23:53,660:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:23:53,664:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:24:03,618:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:24:03,639:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:24:03,639:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:24:03,639:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:24:03,639:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:24:03,640:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:03,641:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:24:03,641:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:03,641:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:24:13,604:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:24:13,666:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:24:13,666:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:24:13,667:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:24:13,667:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:24:13,674:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:13,675:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:24:13,676:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:13,676:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:24:23,636:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:24:23,649:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:24:23,649:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:24:23,650:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:24:23,650:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:24:23,651:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:23,651:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:24:23,651:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:23,652:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:24:33,600:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:24:33,601:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:24:33,601:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:24:33,606:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:24:33,607:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:24:33,611:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:33,611:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:24:33,612:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:33,612:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:24:43,608:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:24:43,626:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:24:43,626:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:24:43,626:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:24:43,627:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:24:43,627:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:43,627:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:24:43,628:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:43,628:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:24:53,621:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:24:53,622:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:24:53,622:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:24:53,622:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:24:53,622:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:24:53,654:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:53,655:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:24:53,655:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:24:53,655:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:25:03,598:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:25:03,608:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:25:03,608:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:25:03,609:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:25:03,609:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:25:03,619:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:03,620:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:25:03,620:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:03,633:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:25:13,591:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:25:13,591:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:25:13,591:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:25:13,592:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:25:13,592:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:25:13,592:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:13,593:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:25:13,593:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:13,593:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:25:23,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:25:23,585:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:25:23,585:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:25:23,585:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:25:23,585:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:25:23,593:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:23,594:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:25:23,594:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:23,595:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:25:33,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:25:33,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:25:33,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:25:33,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:25:33,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:25:33,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:33,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:25:33,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:33,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:25:43,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:25:43,582:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:25:43,582:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:25:43,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:25:43,583:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:25:43,590:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:43,591:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:25:43,592:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:43,592:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:25:53,584:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:25:53,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:25:53,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:25:53,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:25:53,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:25:53,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:53,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:25:53,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:25:53,587:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:26:03,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:26:03,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:26:03,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:26:03,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:26:03,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:26:03,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:03,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:26:03,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:03,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:26:13,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:26:13,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:26:13,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:26:13,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:26:13,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:26:13,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:13,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:26:13,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:13,587:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:26:23,586:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:26:23,586:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:26:23,595:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:26:23,596:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:26:23,596:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:26:23,597:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:23,600:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:26:23,617:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:23,618:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:26:33,616:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:26:33,617:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:26:33,617:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:26:33,617:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:26:33,617:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:26:33,627:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:33,628:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:26:33,628:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:33,628:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:26:43,587:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:26:43,588:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:26:43,588:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:26:43,588:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:26:43,588:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:26:43,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:43,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:26:43,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:43,590:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:26:53,597:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:26:53,598:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:26:53,598:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:26:53,598:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:26:53,598:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:26:53,598:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:53,628:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:26:53,632:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:26:53,633:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:27:03,588:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:27:03,589:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:27:03,589:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:27:03,589:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:27:03,589:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:27:03,598:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:03,598:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:27:03,599:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:03,600:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:27:13,588:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:27:13,589:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:27:13,589:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:27:13,589:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:27:13,589:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:27:13,590:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:13,591:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:27:13,591:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:13,591:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:27:23,613:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:27:23,614:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:27:23,614:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:27:23,614:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:27:23,614:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:27:23,615:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:23,615:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:27:23,615:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:23,615:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:27:33,594:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:27:33,595:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:27:33,595:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:27:33,595:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:27:33,595:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:27:33,596:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:33,610:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:27:33,610:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:33,610:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:27:43,596:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:27:43,596:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:27:43,596:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:27:43,596:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:27:43,596:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:27:43,617:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:43,618:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:27:43,618:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:43,619:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:27:53,597:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:27:53,625:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:27:53,625:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:27:53,626:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:27:53,626:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:27:53,626:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:53,627:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:27:53,645:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:27:53,646:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:28:03,633:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:28:03,635:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:28:03,635:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:28:03,635:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:28:03,635:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:28:03,635:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:03,699:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:28:03,701:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:03,702:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:28:13,608:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:28:13,609:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:28:13,609:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:28:13,609:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:28:13,609:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:28:13,609:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:13,609:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:28:13,609:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:13,610:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:28:23,611:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:28:23,613:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:28:23,613:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:28:23,613:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:28:23,613:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:28:23,614:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:23,614:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:28:23,614:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:23,614:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:28:33,609:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:28:33,612:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:28:33,612:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:28:33,623:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:28:33,624:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:28:33,624:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:33,624:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:28:33,639:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:33,639:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:28:43,634:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:28:43,636:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:28:43,636:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:28:43,636:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:28:43,636:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:28:43,636:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:43,636:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:28:43,636:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:43,637:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:28:53,663:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:28:53,686:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:28:53,686:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:28:53,686:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:28:53,686:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:28:53,688:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:53,689:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:28:53,689:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:28:53,690:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:29:03,588:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:29:03,604:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:29:03,604:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:29:03,604:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:29:03,604:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:29:03,605:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:03,605:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:29:03,605:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:03,605:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:29:13,663:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:29:13,663:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:29:13,663:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:29:13,664:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:29:13,664:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:29:13,664:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:13,664:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:29:13,664:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:13,664:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:29:23,621:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:29:23,623:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:29:23,623:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:29:23,623:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:29:23,623:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:29:23,623:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:23,624:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:29:23,624:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:23,624:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:29:34,488:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:29:34,494:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:29:34,494:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:29:34,494:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:29:34,494:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:29:34,495:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:34,495:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:29:34,556:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:34,557:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:29:43,596:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:29:43,597:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:29:43,597:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:29:43,597:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:29:43,597:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:29:43,648:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:43,658:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:29:43,659:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:43,660:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:29:53,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:29:53,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:29:53,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:29:53,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:29:53,592:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:29:53,593:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:53,593:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:29:53,593:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:29:53,593:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:30:03,585:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:30:03,585:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:30:03,585:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:30:03,585:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:30:03,586:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:30:03,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:03,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:30:03,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:03,587:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:30:13,586:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:30:13,612:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:30:13,612:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:30:13,612:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:30:13,612:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:30:13,612:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:13,613:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:30:13,613:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:13,613:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:30:23,585:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:30:23,585:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:30:23,585:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:30:23,585:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:30:23,586:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:30:23,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:23,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:30:23,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:23,593:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:30:33,584:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:30:33,592:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:30:33,592:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:30:33,592:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:30:33,592:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:30:33,593:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:33,593:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:30:33,599:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:33,600:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:30:43,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:30:43,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:30:43,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:30:43,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:30:43,583:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:30:43,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:43,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:30:43,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:43,584:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:30:43,589:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:42806 in memory (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:43,615:org.apache.spark.ContextCleaner - Cleaned accumulator 1
[INFO ]20161115@20:30:43,629:org.apache.spark.storage.BlockManager - Removing RDD 1
[INFO ]20161115@20:30:43,653:org.apache.spark.ContextCleaner - Cleaned RDD 1
[INFO ]20161115@20:30:43,656:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on localhost:42806 in memory (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@20:30:53,598:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:30:53,599:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:30:53,599:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:30:53,599:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:30:53,599:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:30:53,599:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:53,599:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:30:53,600:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:30:53,600:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:31:03,585:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:31:03,585:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:31:03,585:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:31:03,585:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:31:03,585:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:31:03,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:03,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:31:03,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:03,586:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:31:13,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:31:13,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:31:13,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:31:13,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:31:13,583:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:31:13,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:13,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:31:13,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:13,587:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:31:23,584:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:31:23,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:31:23,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:31:23,585:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:31:23,585:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:31:23,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:23,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:31:23,588:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:23,588:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:31:33,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:31:33,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:31:33,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:31:33,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:31:33,583:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:31:33,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:33,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:31:33,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:33,586:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:31:43,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:31:43,586:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:31:43,587:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:31:43,587:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:31:43,587:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:31:43,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:43,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:31:43,588:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:43,588:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:31:53,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:31:53,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:31:53,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:31:53,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:31:53,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:31:53,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:53,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:31:53,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:31:53,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:32:03,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:32:03,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:32:03,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:32:03,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:32:03,583:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:32:03,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:03,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:32:03,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:03,584:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:32:13,584:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:32:13,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:32:13,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:32:13,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:32:13,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:32:13,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:13,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:32:13,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:13,586:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:32:23,584:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:32:23,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:32:23,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:32:23,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:32:23,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:32:23,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:23,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:32:23,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:23,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:32:33,584:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:32:33,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:32:33,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:32:33,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:32:33,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:32:33,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:33,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:32:33,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:33,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:32:43,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:32:43,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:32:43,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:32:43,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:32:43,583:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:32:43,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:43,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:32:43,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:43,584:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:32:53,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:32:53,582:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:32:53,582:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:32:53,582:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:32:53,582:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:32:53,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:53,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:32:53,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:32:53,583:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:33:03,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:33:03,582:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:33:03,582:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:33:03,582:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:33:03,582:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:33:03,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:03,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:33:03,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:03,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:33:13,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:33:13,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:33:13,585:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:33:13,585:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:33:13,585:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:33:13,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:13,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:33:13,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:13,589:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:33:23,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:33:23,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:33:23,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:33:23,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:33:23,583:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:33:23,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:23,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:33:23,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:23,584:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:33:33,585:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:33:33,585:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:33:33,585:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:33:33,585:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:33:33,585:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:33:33,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:33,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:33:33,586:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:33,586:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:33:43,584:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:33:43,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:33:43,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:33:43,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:33:43,585:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:33:43,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:43,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:33:43,587:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:43,587:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:33:53,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:33:53,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:33:53,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:33:53,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:33:53,583:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:33:53,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:53,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:33:53,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:33:53,584:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:34:03,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:34:03,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:34:03,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:34:03,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:34:03,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:34:03,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:03,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:34:03,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:03,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:34:13,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:34:13,582:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:34:13,582:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:34:13,582:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:34:13,582:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:34:13,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:13,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:34:13,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:13,583:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:34:23,584:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:34:23,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:34:23,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:34:23,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:34:23,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:34:23,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:23,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:34:23,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:23,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:34:33,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:34:33,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:34:33,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:34:33,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:34:33,583:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:34:33,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:33,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:34:33,585:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:33,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:34:43,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:34:43,582:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:34:43,582:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:34:43,582:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:34:43,582:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:34:43,582:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:43,582:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:34:43,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:43,583:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:34:53,582:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:34:53,583:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:34:53,583:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:34:53,583:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:34:53,583:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:34:53,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:53,583:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:34:53,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:34:53,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:35:03,584:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:35:03,598:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:35:03,598:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:35:03,599:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:35:03,599:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:35:03,599:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:03,599:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:35:03,600:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:03,600:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:35:13,588:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:35:13,588:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:35:13,588:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:35:13,589:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:35:13,589:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:35:13,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:13,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:35:13,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:13,589:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:35:23,583:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:35:23,584:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:35:23,584:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:35:23,584:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:35:23,584:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:35:23,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:23,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:35:23,584:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:23,585:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:35:33,587:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:35:33,588:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:35:33,588:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:35:33,588:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:35:33,588:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:35:33,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:33,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:35:33,589:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:33,589:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:35:43,591:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:35:43,592:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:35:43,592:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:35:43,592:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:35:43,592:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:35:43,592:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:43,593:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:35:43,593:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:43,593:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@20:35:53,585:org.apache.spark.executor.Executor - Told to re-register on heartbeat
[INFO ]20161115@20:35:53,594:org.apache.spark.storage.BlockManager - BlockManager re-registering with master
[INFO ]20161115@20:35:53,594:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@20:35:53,594:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@20:35:53,594:org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
[INFO ]20161115@20:35:53,595:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:33955 (size: 1980.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:53,595:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:33955 (size: 14.9 KB, free: 499.9 MB)
[INFO ]20161115@20:35:53,595:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:33955 (size: 1859.0 B, free: 499.9 MB)
[INFO ]20161115@20:35:53,595:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:33955 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@21:29:51,392:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@21:29:53,035:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@21:29:55,001:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@21:29:55,003:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@21:29:55,003:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@21:29:56,694:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34060.
[INFO ]20161115@21:29:58,533:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@21:29:59,152:Remoting - Starting remoting
[INFO ]20161115@21:30:01,128:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:54003]
[INFO ]20161115@21:30:01,175:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:54003]
[INFO ]20161115@21:30:01,451:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 54003.
[INFO ]20161115@21:30:01,818:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@21:30:01,931:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@21:30:01,963:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-ff7e2c91-1682-425c-b988-6df0fec3806a
[INFO ]20161115@21:30:02,022:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@21:30:02,242:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@21:30:03,403:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@21:30:03,453:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@21:30:04,135:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@21:30:05,613:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41033.
[INFO ]20161115@21:30:05,614:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 41033
[INFO ]20161115@21:30:05,616:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@21:30:05,619:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:41033 with 500.1 MB RAM, BlockManagerId(driver, localhost, 41033)
[INFO ]20161115@21:30:05,665:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@21:30:08,376:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@21:30:10,082:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@21:30:10,090:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:41033 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@21:30:10,304:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:162
[INFO ]20161115@21:30:11,999:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@21:30:12,433:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:175
[INFO ]20161115@21:30:12,550:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:175) with 1 output partitions
[INFO ]20161115@21:30:12,550:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:175)
[INFO ]20161115@21:30:12,552:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@21:30:12,556:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:30:12,795:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:162), which has no missing parents
[INFO ]20161115@21:30:13,104:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@21:30:13,113:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@21:30:13,114:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:41033 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@21:30:13,115:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:30:13,181:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:162)
[INFO ]20161115@21:30:13,187:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@21:30:13,796:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@21:30:13,865:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@21:30:13,905:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@21:30:13,909:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@21:30:13,929:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@21:30:13,929:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@21:30:13,929:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@21:30:13,929:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@21:30:13,929:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@21:30:14,089:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@21:30:14,134:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:41033 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@21:30:14,315:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@21:30:14,422:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 859 ms on localhost (1/1)
[INFO ]20161115@21:30:14,574:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:175) finished in 1.269 s
[INFO ]20161115@21:30:15,131:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:30:15,168:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:175, took 2.726576 s
[INFO ]20161115@21:30:27,657:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161115@21:30:27,748:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@21:30:27,813:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@21:30:27,843:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@21:30:27,859:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@21:30:27,861:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@21:30:27,868:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@21:30:27,896:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@21:30:27,896:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@21:30:27,897:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-3465c137-75ea-4d59-80f4-5ebaa80d9a76
[INFO ]20161115@21:31:52,529:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@21:31:53,091:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@21:31:53,546:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@21:31:53,547:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@21:31:53,548:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@21:31:53,969:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 52743.
[INFO ]20161115@21:31:54,564:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@21:31:54,629:Remoting - Starting remoting
[INFO ]20161115@21:31:54,961:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:48421]
[INFO ]20161115@21:31:54,964:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:48421]
[INFO ]20161115@21:31:54,989:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 48421.
[INFO ]20161115@21:31:55,045:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@21:31:55,099:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@21:31:55,132:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-0ca50fa6-e819-4701-86b6-fbbedc517c75
[INFO ]20161115@21:31:55,149:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@21:31:55,268:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@21:31:55,758:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@21:31:55,760:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@21:31:55,939:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@21:31:55,973:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59069.
[INFO ]20161115@21:31:55,974:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 59069
[INFO ]20161115@21:31:55,975:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@21:31:55,977:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:59069 with 500.1 MB RAM, BlockManagerId(driver, localhost, 59069)
[INFO ]20161115@21:31:55,982:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@21:31:57,410:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@21:31:57,914:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@21:31:57,916:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:59069 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@21:31:57,934:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:162
[INFO ]20161115@21:31:58,603:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@21:31:58,767:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:175
[INFO ]20161115@21:31:58,817:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:175) with 1 output partitions
[INFO ]20161115@21:31:58,818:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:175)
[INFO ]20161115@21:31:58,819:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@21:31:58,826:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:31:58,875:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:162), which has no missing parents
[INFO ]20161115@21:31:58,940:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@21:31:58,963:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@21:31:58,965:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:59069 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@21:31:58,966:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:31:58,972:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:162)
[INFO ]20161115@21:31:58,986:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@21:31:59,148:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@21:31:59,182:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@21:31:59,234:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@21:31:59,238:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@21:31:59,270:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@21:31:59,270:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@21:31:59,270:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@21:31:59,270:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@21:31:59,271:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@21:31:59,417:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@21:31:59,419:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:59069 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@21:31:59,527:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@21:31:59,593:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:175) finished in 0.576 s
[INFO ]20161115@21:31:59,594:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 515 ms on localhost (1/1)
[INFO ]20161115@21:31:59,602:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:31:59,614:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:175, took 0.839148 s
[INFO ]20161115@21:32:14,916:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:233
[INFO ]20161115@21:32:14,919:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:233) with 1 output partitions
[INFO ]20161115@21:32:14,919:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:233)
[INFO ]20161115@21:32:14,920:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@21:32:15,049:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:32:15,051:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at flatMap at HW2_Part1.java:194), which has no missing parents
[INFO ]20161115@21:32:15,080:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.6 KB, free 343.8 KB)
[INFO ]20161115@21:32:15,098:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 345.9 KB)
[INFO ]20161115@21:32:15,099:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:59069 (size: 2.2 KB, free: 499.9 MB)
[INFO ]20161115@21:32:15,100:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:32:15,100:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at flatMap at HW2_Part1.java:194)
[INFO ]20161115@21:32:15,100:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@21:32:15,243:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@21:32:15,247:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@21:32:15,461:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@21:32:54,845:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@21:32:55,322:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@21:32:55,787:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@21:32:55,788:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@21:32:55,789:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@21:32:56,332:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56338.
[INFO ]20161115@21:32:56,948:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@21:32:57,033:Remoting - Starting remoting
[INFO ]20161115@21:32:57,358:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:59951]
[INFO ]20161115@21:32:57,360:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:59951]
[INFO ]20161115@21:32:57,370:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 59951.
[INFO ]20161115@21:32:57,428:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@21:32:57,472:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@21:32:57,516:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-263f4608-bec5-42d6-a5a8-bf3d869a7888
[INFO ]20161115@21:32:57,569:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@21:32:57,730:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@21:32:58,560:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@21:32:58,562:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@21:32:58,961:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@21:32:59,058:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50624.
[INFO ]20161115@21:32:59,059:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 50624
[INFO ]20161115@21:32:59,061:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@21:32:59,075:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:50624 with 500.1 MB RAM, BlockManagerId(driver, localhost, 50624)
[INFO ]20161115@21:32:59,080:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@21:33:00,887:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@21:33:01,552:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@21:33:01,566:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:50624 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@21:33:01,576:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:162
[INFO ]20161115@21:33:02,726:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@21:33:02,944:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:175
[INFO ]20161115@21:33:02,986:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:175) with 1 output partitions
[INFO ]20161115@21:33:02,987:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:175)
[INFO ]20161115@21:33:02,989:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@21:33:03,008:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:33:03,401:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:162), which has no missing parents
[INFO ]20161115@21:33:03,593:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@21:33:03,833:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@21:33:03,835:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:50624 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@21:33:03,837:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:33:03,842:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:162)
[INFO ]20161115@21:33:03,845:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@21:33:03,988:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@21:33:04,049:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@21:33:04,163:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@21:33:04,199:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@21:33:04,402:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@21:33:04,411:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@21:33:04,411:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@21:33:04,411:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@21:33:04,412:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@21:33:04,541:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@21:33:04,544:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:50624 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@21:33:04,886:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@21:33:04,972:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 984 ms on localhost (1/1)
[INFO ]20161115@21:33:04,975:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:175) finished in 1.116 s
[INFO ]20161115@21:33:05,000:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:33:05,020:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:175, took 2.070310 s
[INFO ]20161115@21:33:10,494:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:233
[INFO ]20161115@21:33:10,497:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:233) with 1 output partitions
[INFO ]20161115@21:33:10,497:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:233)
[INFO ]20161115@21:33:10,528:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@21:33:10,569:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:33:10,579:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at flatMap at HW2_Part1.java:194), which has no missing parents
[INFO ]20161115@21:33:10,589:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.6 KB, free 343.8 KB)
[INFO ]20161115@21:33:10,596:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 345.9 KB)
[INFO ]20161115@21:33:10,597:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:50624 (size: 2.2 KB, free: 499.9 MB)
[INFO ]20161115@21:33:10,598:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:33:10,598:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at flatMap at HW2_Part1.java:194)
[INFO ]20161115@21:33:10,598:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@21:33:10,636:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@21:33:10,637:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@21:33:10,761:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@21:34:37,033:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2003 bytes result sent to driver
[INFO ]20161115@21:34:37,118:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:233) finished in 86.516 s
[INFO ]20161115@21:34:37,119:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:233, took 86.623422 s
[INFO ]20161115@21:34:37,120:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 86516 ms on localhost (1/1)
[INFO ]20161115@21:34:37,123:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:34:37,189:homework2.HW2_Part1 - --->Number of valid records for SP500:  505
[INFO ]20161115@21:34:44,902:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161115@21:34:45,189:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part1.java:303
[INFO ]20161115@21:34:45,225:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:34:45,226:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part1.java:303) with 1 output partitions
[INFO ]20161115@21:34:45,226:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (saveAsTextFile at HW2_Part1.java:303)
[INFO ]20161115@21:34:45,226:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161115@21:34:45,227:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161115@21:34:45,228:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:289), which has no missing parents
[INFO ]20161115@21:34:45,237:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.5 KB, free 351.4 KB)
[INFO ]20161115@21:34:45,244:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.0 KB, free 354.5 KB)
[INFO ]20161115@21:34:45,245:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:50624 (size: 3.0 KB, free: 499.9 MB)
[INFO ]20161115@21:34:45,245:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:34:45,247:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:34:45,247:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161115@21:34:45,249:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161115@21:34:45,249:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161115@21:34:45,398:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[ERROR]20161115@21:35:07,471:homework2.HW2_Part1 - NumberFormatException:
[INFO ]20161115@21:35:47,818:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2182 bytes result sent to driver
[INFO ]20161115@21:35:47,877:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 62626 ms on localhost (1/1)
[INFO ]20161115@21:35:47,878:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:35:47,890:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part1.java:289) finished in 62.641 s
[INFO ]20161115@21:35:47,899:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161115@21:35:47,900:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161115@21:35:47,900:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
[INFO ]20161115@21:35:47,901:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161115@21:35:47,909:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:303), which has no missing parents
[INFO ]20161115@21:35:47,954:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 71.4 KB, free 425.8 KB)
[INFO ]20161115@21:35:47,990:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.0 KB, free 450.8 KB)
[INFO ]20161115@21:35:47,992:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:50624 (size: 25.0 KB, free: 499.9 MB)
[INFO ]20161115@21:35:47,994:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:35:47,995:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:303)
[INFO ]20161115@21:35:47,998:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161115@21:35:48,019:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@21:35:48,019:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161115@21:35:48,266:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@21:35:48,353:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:50624 in memory (size: 3.0 KB, free: 499.9 MB)
[INFO ]20161115@21:35:48,354:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 97 ms
[INFO ]20161115@21:35:48,643:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161115@21:35:48,795:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611152134_0003_m_000000_3' to file:/home/cloudera/git/siming.meng/homework2/output/HW2Part1_1479274374115/_temporary/0/task_201611152134_0003_m_000000
[INFO ]20161115@21:35:48,801:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611152134_0003_m_000000_3: Committed
[INFO ]20161115@21:35:48,823:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2038 bytes result sent to driver
[INFO ]20161115@21:35:48,846:org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (saveAsTextFile at HW2_Part1.java:303) finished in 0.842 s
[INFO ]20161115@21:35:48,854:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part1.java:303, took 63.664119 s
[INFO ]20161115@21:35:48,860:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 841 ms on localhost (1/1)
[INFO ]20161115@21:35:48,860:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:35:51,834:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:312
[INFO ]20161115@21:35:51,949:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161115@21:35:51,954:org.apache.spark.scheduler.DAGScheduler - Got job 3 (count at HW2_Part1.java:312) with 1 output partitions
[INFO ]20161115@21:35:51,954:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at HW2_Part1.java:312)
[INFO ]20161115@21:35:51,954:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
[INFO ]20161115@21:35:51,955:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:35:51,955:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:289), which has no missing parents
[INFO ]20161115@21:35:51,960:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.5 KB, free 445.7 KB)
[INFO ]20161115@21:35:52,015:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.0 KB, free 447.8 KB)
[INFO ]20161115@21:35:52,016:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:50624 (size: 2.0 KB, free: 499.9 MB)
[INFO ]20161115@21:35:52,019:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:35:52,020:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:35:52,020:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161115@21:35:52,021:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@21:35:52,021:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
[INFO ]20161115@21:35:52,072:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161115@21:35:52,072:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161115@21:35:52,073:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:50624 in memory (size: 25.0 KB, free: 499.9 MB)
[INFO ]20161115@21:35:52,078:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@21:35:52,096:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 17 ms
[INFO ]20161115@21:35:52,203:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 1124 bytes result sent to driver
[INFO ]20161115@21:35:52,213:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (count at HW2_Part1.java:312) finished in 0.191 s
[INFO ]20161115@21:35:52,214:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: count at HW2_Part1.java:312, took 0.371694 s
[INFO ]20161115@21:35:52,214:homework2.HW2_Part1 - Valid records:  504
[INFO ]20161115@21:35:52,223:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 187 ms on localhost (1/1)
[INFO ]20161115@21:35:52,223:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:35:52,316:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@21:35:52,372:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@21:35:52,416:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@21:35:52,418:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@21:35:52,421:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@21:35:52,426:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@21:35:52,459:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@21:35:52,467:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@21:35:52,480:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161115@21:35:52,687:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@21:35:52,690:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-d4b13e98-bc3b-4f4e-af64-225e914eacf2
[INFO ]20161115@21:45:34,223:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@21:45:34,771:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@21:45:35,364:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@21:45:35,365:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@21:45:35,366:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@21:45:36,014:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37206.
[INFO ]20161115@21:45:36,969:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@21:45:37,406:Remoting - Starting remoting
[INFO ]20161115@21:45:38,265:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:54359]
[INFO ]20161115@21:45:38,274:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:54359]
[INFO ]20161115@21:45:38,485:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 54359.
[INFO ]20161115@21:45:38,753:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@21:45:39,147:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@21:45:39,218:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f4f67bd1-11a2-41ef-ac4b-2d971dffa406
[INFO ]20161115@21:45:39,285:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@21:45:39,977:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@21:45:41,292:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@21:45:41,294:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@21:45:42,393:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@21:45:42,428:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49691.
[INFO ]20161115@21:45:42,430:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 49691
[INFO ]20161115@21:45:42,431:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@21:45:42,435:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:49691 with 500.1 MB RAM, BlockManagerId(driver, localhost, 49691)
[INFO ]20161115@21:45:42,451:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@21:45:44,937:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@21:45:45,785:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@21:45:45,789:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:49691 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@21:45:45,812:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:162
[INFO ]20161115@21:45:46,726:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@21:45:46,794:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:175
[INFO ]20161115@21:45:47,140:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:175) with 1 output partitions
[INFO ]20161115@21:45:47,140:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:175)
[INFO ]20161115@21:45:47,141:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@21:45:47,170:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:45:47,350:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:162), which has no missing parents
[INFO ]20161115@21:45:47,395:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@21:45:47,527:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@21:45:47,528:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:49691 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@21:45:47,529:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:45:47,535:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:162)
[INFO ]20161115@21:45:47,538:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@21:45:47,982:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@21:45:48,108:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@21:45:48,149:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@21:45:48,207:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@21:45:48,257:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@21:45:48,263:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@21:45:48,263:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@21:45:48,264:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@21:45:48,264:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@21:45:48,428:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@21:45:48,442:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:49691 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@21:45:48,738:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@21:45:49,105:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1184 ms on localhost (1/1)
[INFO ]20161115@21:45:49,110:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:175) finished in 1.555 s
[INFO ]20161115@21:45:49,112:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:45:49,156:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:175, took 2.356517 s
[INFO ]20161115@21:46:01,625:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:233
[INFO ]20161115@21:46:01,628:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:233) with 1 output partitions
[INFO ]20161115@21:46:01,628:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:233)
[INFO ]20161115@21:46:01,629:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@21:46:01,637:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:46:01,637:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at flatMap at HW2_Part1.java:194), which has no missing parents
[INFO ]20161115@21:46:01,682:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.6 KB, free 343.8 KB)
[INFO ]20161115@21:46:01,800:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 345.9 KB)
[INFO ]20161115@21:46:01,800:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:49691 (size: 2.2 KB, free: 499.9 MB)
[INFO ]20161115@21:46:01,804:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:46:01,804:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at flatMap at HW2_Part1.java:194)
[INFO ]20161115@21:46:01,805:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@21:46:01,815:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@21:46:01,815:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@21:46:01,844:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@21:46:02,048:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2003 bytes result sent to driver
[INFO ]20161115@21:46:02,111:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:233) finished in 0.298 s
[INFO ]20161115@21:46:02,112:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:233, took 0.484965 s
[INFO ]20161115@21:46:02,113:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 298 ms on localhost (1/1)
[INFO ]20161115@21:46:02,113:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:46:02,132:homework2.HW2_Part1 - --->Number of valid records for SP500:  505
[INFO ]20161115@21:46:08,014:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161115@21:46:08,333:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part1.java:303
[INFO ]20161115@21:46:08,384:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:46:08,387:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part1.java:303) with 1 output partitions
[INFO ]20161115@21:46:08,388:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (saveAsTextFile at HW2_Part1.java:303)
[INFO ]20161115@21:46:08,388:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161115@21:46:08,388:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161115@21:46:08,391:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:289), which has no missing parents
[INFO ]20161115@21:46:08,436:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.5 KB, free 351.4 KB)
[INFO ]20161115@21:46:08,465:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.0 KB, free 354.5 KB)
[INFO ]20161115@21:46:08,466:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:49691 (size: 3.0 KB, free: 499.9 MB)
[INFO ]20161115@21:46:08,467:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:46:08,470:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:46:08,474:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161115@21:46:08,482:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161115@21:46:08,522:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161115@21:46:08,679:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[ERROR]20161115@21:46:13,899:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:15,190:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:16,353:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:17,378:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:29,543:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:30,951:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:31,912:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:32,986:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:33,934:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:34,852:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:35,993:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:37,576:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:38,662:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:39,606:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:40,871:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:42,067:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:43,201:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:44,165:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:45,354:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:46,417:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:47,496:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:48,336:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:49,337:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:50,133:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:50,954:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:51,865:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:56,622:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:46:57,805:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,012:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,035:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,036:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,036:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,036:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,039:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,040:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,040:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,040:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,040:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,041:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,042:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,042:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,042:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,042:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,250:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,258:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,258:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,261:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,261:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,261:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,261:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,266:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,266:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,267:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,270:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,270:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,270:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,270:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,271:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,273:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,273:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,279:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,281:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,282:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,282:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,284:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,285:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,285:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,285:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,285:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:47:02,285:homework2.HW2_Part1 - NumberFormatException:
[INFO ]20161115@21:47:02,528:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2182 bytes result sent to driver
[INFO ]20161115@21:47:02,560:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 54082 ms on localhost (1/1)
[INFO ]20161115@21:47:02,560:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:47:02,569:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part1.java:289) finished in 54.086 s
[INFO ]20161115@21:47:02,601:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161115@21:47:02,602:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161115@21:47:02,605:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
[INFO ]20161115@21:47:02,606:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161115@21:47:02,624:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:303), which has no missing parents
[INFO ]20161115@21:47:02,746:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 71.4 KB, free 425.8 KB)
[INFO ]20161115@21:47:02,762:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.0 KB, free 450.8 KB)
[INFO ]20161115@21:47:02,763:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:49691 (size: 25.0 KB, free: 499.9 MB)
[INFO ]20161115@21:47:02,764:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:47:02,764:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:303)
[INFO ]20161115@21:47:02,764:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161115@21:47:02,778:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@21:47:02,778:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161115@21:47:03,014:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@21:47:03,024:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 88 ms
[INFO ]20161115@21:47:03,075:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:49691 in memory (size: 3.0 KB, free: 499.9 MB)
[INFO ]20161115@21:47:03,365:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161115@21:47:03,602:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611152146_0003_m_000000_3' to file:/home/cloudera/git/siming.meng/homework2/output/HW2Part1_1479275133561/_temporary/0/task_201611152146_0003_m_000000
[INFO ]20161115@21:47:03,603:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611152146_0003_m_000000_3: Committed
[INFO ]20161115@21:47:03,610:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2038 bytes result sent to driver
[INFO ]20161115@21:47:03,615:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 849 ms on localhost (1/1)
[INFO ]20161115@21:47:03,616:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:47:03,657:org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (saveAsTextFile at HW2_Part1.java:303) finished in 0.891 s
[INFO ]20161115@21:47:03,661:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part1.java:303, took 55.322604 s
[INFO ]20161115@21:47:27,056:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:312
[INFO ]20161115@21:47:27,319:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161115@21:47:27,322:org.apache.spark.scheduler.DAGScheduler - Got job 3 (count at HW2_Part1.java:312) with 1 output partitions
[INFO ]20161115@21:47:27,322:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at HW2_Part1.java:312)
[INFO ]20161115@21:47:27,322:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
[INFO ]20161115@21:47:27,323:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:47:27,325:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:289), which has no missing parents
[INFO ]20161115@21:47:27,338:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.5 KB, free 445.7 KB)
[INFO ]20161115@21:47:27,396:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161115@21:47:27,397:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:49691 in memory (size: 25.0 KB, free: 499.9 MB)
[INFO ]20161115@21:47:27,400:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.0 KB, free 351.4 KB)
[INFO ]20161115@21:47:27,406:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:49691 (size: 2.0 KB, free: 499.9 MB)
[INFO ]20161115@21:47:27,408:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:47:27,409:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:47:27,409:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161115@21:47:27,410:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@21:47:27,411:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
[INFO ]20161115@21:47:27,431:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@21:47:27,431:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[INFO ]20161115@21:47:27,535:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 1124 bytes result sent to driver
[INFO ]20161115@21:47:27,539:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (count at HW2_Part1.java:312) finished in 0.125 s
[INFO ]20161115@21:47:27,540:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: count at HW2_Part1.java:312, took 0.481856 s
[INFO ]20161115@21:47:27,540:homework2.HW2_Part1 - Valid records:  435
[INFO ]20161115@21:47:27,542:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 129 ms on localhost (1/1)
[INFO ]20161115@21:47:27,542:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:313
[INFO ]20161115@21:47:27,543:org.apache.spark.scheduler.DAGScheduler - Got job 4 (count at HW2_Part1.java:313) with 1 output partitions
[INFO ]20161115@21:47:27,543:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (count at HW2_Part1.java:313)
[INFO ]20161115@21:47:27,543:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
[INFO ]20161115@21:47:27,543:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:47:27,544:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:289), which has no missing parents
[INFO ]20161115@21:47:27,547:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.5 KB, free 354.9 KB)
[INFO ]20161115@21:47:27,550:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:47:27,557:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161115@21:47:27,559:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:49691 in memory (size: 2.0 KB, free: 499.9 MB)
[INFO ]20161115@21:47:27,578:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.0 KB, free 351.4 KB)
[INFO ]20161115@21:47:27,582:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:49691 (size: 2.0 KB, free: 499.9 MB)
[INFO ]20161115@21:47:27,585:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:47:27,586:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:47:27,587:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
[INFO ]20161115@21:47:27,588:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 5, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@21:47:27,588:org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 5)
[INFO ]20161115@21:47:27,659:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@21:47:27,667:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 8 ms
[INFO ]20161115@21:47:27,755:org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 5). 1124 bytes result sent to driver
[INFO ]20161115@21:47:27,769:org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (count at HW2_Part1.java:313) finished in 0.173 s
[INFO ]20161115@21:47:27,771:org.apache.spark.scheduler.DAGScheduler - Job 4 finished: count at HW2_Part1.java:313, took 0.228288 s
[INFO ]20161115@21:47:27,777:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 5) in 177 ms on localhost (1/1)
[INFO ]20161115@21:47:27,778:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:47:28,041:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@21:47:28,166:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@21:47:28,201:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@21:47:28,203:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@21:47:28,207:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@21:47:28,210:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@21:47:28,228:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@21:47:28,294:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@21:47:28,295:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-46512dbe-5724-4b6f-b70e-b4584e70b648
[INFO ]20161115@21:47:28,343:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@21:47:28,349:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161115@21:48:47,917:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@21:48:48,590:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@21:48:49,006:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@21:48:49,006:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@21:48:49,007:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@21:48:49,445:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 47751.
[INFO ]20161115@21:48:49,950:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@21:48:50,023:Remoting - Starting remoting
[INFO ]20161115@21:48:50,407:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:35626]
[INFO ]20161115@21:48:50,408:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:35626]
[INFO ]20161115@21:48:50,437:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 35626.
[INFO ]20161115@21:48:50,538:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@21:48:50,605:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@21:48:50,635:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-8df917f7-4075-4df6-b94e-52cafc77756b
[INFO ]20161115@21:48:50,677:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@21:48:50,805:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@21:48:51,427:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@21:48:51,429:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@21:48:51,666:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@21:48:51,695:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58019.
[INFO ]20161115@21:48:51,695:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 58019
[INFO ]20161115@21:48:51,696:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@21:48:51,700:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:58019 with 500.1 MB RAM, BlockManagerId(driver, localhost, 58019)
[INFO ]20161115@21:48:51,709:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@21:48:52,728:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@21:48:53,166:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@21:48:53,169:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:58019 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@21:48:53,200:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:162
[INFO ]20161115@21:48:53,866:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@21:48:54,051:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:175
[INFO ]20161115@21:48:54,093:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:175) with 1 output partitions
[INFO ]20161115@21:48:54,093:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:175)
[INFO ]20161115@21:48:54,094:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@21:48:54,098:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:48:54,138:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:162), which has no missing parents
[INFO ]20161115@21:48:54,229:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@21:48:54,267:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@21:48:54,268:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:58019 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@21:48:54,269:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:48:54,273:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:162)
[INFO ]20161115@21:48:54,276:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@21:48:54,379:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@21:48:54,402:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@21:48:54,505:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@21:48:54,537:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@21:48:54,567:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@21:48:54,567:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@21:48:54,567:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@21:48:54,568:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@21:48:54,568:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@21:48:54,703:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@21:48:54,704:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:58019 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@21:48:54,860:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@21:48:55,005:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 596 ms on localhost (1/1)
[INFO ]20161115@21:48:55,007:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:48:55,069:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:175) finished in 0.777 s
[INFO ]20161115@21:48:55,089:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:175, took 1.032456 s
[INFO ]20161115@21:49:10,191:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:233
[INFO ]20161115@21:49:10,193:org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at HW2_Part1.java:233) with 1 output partitions
[INFO ]20161115@21:49:10,203:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (count at HW2_Part1.java:233)
[INFO ]20161115@21:49:10,203:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@21:49:10,306:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:49:10,309:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[2] at flatMap at HW2_Part1.java:194), which has no missing parents
[INFO ]20161115@21:49:10,336:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.6 KB, free 343.8 KB)
[INFO ]20161115@21:49:10,364:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 345.9 KB)
[INFO ]20161115@21:49:10,365:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:58019 (size: 2.2 KB, free: 499.9 MB)
[INFO ]20161115@21:49:10,365:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:49:10,366:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at flatMap at HW2_Part1.java:194)
[INFO ]20161115@21:49:10,366:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@21:49:10,426:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@21:49:10,434:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@21:49:10,497:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[INFO ]20161115@21:49:10,617:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2003 bytes result sent to driver
[INFO ]20161115@21:49:10,749:org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (count at HW2_Part1.java:233) finished in 0.376 s
[INFO ]20161115@21:49:10,749:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at HW2_Part1.java:233, took 0.556668 s
[INFO ]20161115@21:49:10,750:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 375 ms on localhost (1/1)
[INFO ]20161115@21:49:10,750:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:49:10,775:homework2.HW2_Part1 - --->Number of valid records for SP500:  505
[INFO ]20161115@21:49:25,973:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161115@21:49:26,323:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part1.java:303
[INFO ]20161115@21:49:26,382:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:49:26,384:org.apache.spark.scheduler.DAGScheduler - Got job 2 (saveAsTextFile at HW2_Part1.java:303) with 1 output partitions
[INFO ]20161115@21:49:26,384:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (saveAsTextFile at HW2_Part1.java:303)
[INFO ]20161115@21:49:26,384:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
[INFO ]20161115@21:49:26,385:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
[INFO ]20161115@21:49:26,387:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:289), which has no missing parents
[INFO ]20161115@21:49:26,421:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.5 KB, free 351.4 KB)
[INFO ]20161115@21:49:26,527:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.0 KB, free 354.5 KB)
[INFO ]20161115@21:49:26,529:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:58019 (size: 3.0 KB, free: 499.9 MB)
[INFO ]20161115@21:49:26,558:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:49:26,565:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:49:26,565:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161115@21:49:26,567:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161115@21:49:26,567:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161115@21:49:26,608:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[ERROR]20161115@21:49:26,610:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,665:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,667:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,691:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,691:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,692:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,693:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,693:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,694:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,700:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,703:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,704:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,710:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,711:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,715:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,716:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,716:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,719:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,720:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,720:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,720:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,721:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,732:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,733:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,734:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,735:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,736:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,737:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,739:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,741:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,741:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,742:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,744:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,762:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,770:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,774:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,779:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,781:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,781:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,781:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,782:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,910:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,911:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,911:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,913:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,922:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,925:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,926:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,928:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,936:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,939:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,941:homework2.HW2_Part1 - NumberFormatException:
[ERROR]20161115@21:49:26,942:homework2.HW2_Part1 - NumberFormatException:
[INFO ]20161115@21:49:27,624:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2182 bytes result sent to driver
[INFO ]20161115@21:49:27,676:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 1107 ms on localhost (1/1)
[INFO ]20161115@21:49:27,676:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:49:27,677:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (sortBy at HW2_Part1.java:289) finished in 1.105 s
[INFO ]20161115@21:49:27,693:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161115@21:49:27,694:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161115@21:49:27,695:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
[INFO ]20161115@21:49:27,695:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161115@21:49:27,698:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:303), which has no missing parents
[INFO ]20161115@21:49:27,759:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 71.4 KB, free 425.8 KB)
[INFO ]20161115@21:49:27,860:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.0 KB, free 450.8 KB)
[INFO ]20161115@21:49:27,863:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:58019 (size: 25.0 KB, free: 499.9 MB)
[INFO ]20161115@21:49:27,865:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:49:27,865:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:303)
[INFO ]20161115@21:49:27,865:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
[INFO ]20161115@21:49:27,869:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@21:49:27,869:org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
[INFO ]20161115@21:49:28,052:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@21:49:28,055:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
[INFO ]20161115@21:49:28,391:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:58019 in memory (size: 3.0 KB, free: 499.9 MB)
[INFO ]20161115@21:49:28,586:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161115@21:49:28,741:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611152149_0003_m_000000_3' to file:/home/cloudera/git/siming.meng/homework2/output/HW2Part1_1479275327222/_temporary/0/task_201611152149_0003_m_000000
[INFO ]20161115@21:49:28,742:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611152149_0003_m_000000_3: Committed
[INFO ]20161115@21:49:28,751:org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2038 bytes result sent to driver
[INFO ]20161115@21:49:28,772:org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (saveAsTextFile at HW2_Part1.java:303) finished in 0.898 s
[INFO ]20161115@21:49:28,773:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: saveAsTextFile at HW2_Part1.java:303, took 2.449784 s
[INFO ]20161115@21:49:28,779:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 897 ms on localhost (1/1)
[INFO ]20161115@21:49:28,780:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:49:32,987:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:312
[INFO ]20161115@21:49:33,048:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161115@21:49:33,055:org.apache.spark.scheduler.DAGScheduler - Got job 3 (count at HW2_Part1.java:312) with 1 output partitions
[INFO ]20161115@21:49:33,055:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (count at HW2_Part1.java:312)
[INFO ]20161115@21:49:33,055:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
[INFO ]20161115@21:49:33,056:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:49:33,057:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:289), which has no missing parents
[INFO ]20161115@21:49:33,069:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.5 KB, free 445.7 KB)
[INFO ]20161115@21:49:33,102:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.0 KB, free 447.8 KB)
[INFO ]20161115@21:49:33,105:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:58019 (size: 2.0 KB, free: 499.9 MB)
[INFO ]20161115@21:49:33,107:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:49:33,107:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:49:33,107:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161115@21:49:33,108:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@21:49:33,108:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
[INFO ]20161115@21:49:33,121:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@21:49:33,131:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 10 ms
[INFO ]20161115@21:49:33,273:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 1124 bytes result sent to driver
[INFO ]20161115@21:49:33,298:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (count at HW2_Part1.java:312) finished in 0.183 s
[INFO ]20161115@21:49:33,300:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: count at HW2_Part1.java:312, took 0.310832 s
[INFO ]20161115@21:49:33,300:homework2.HW2_Part1 - Valid records:  452
[INFO ]20161115@21:49:33,302:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:313
[INFO ]20161115@21:49:33,304:org.apache.spark.scheduler.DAGScheduler - Got job 4 (count at HW2_Part1.java:313) with 1 output partitions
[INFO ]20161115@21:49:33,305:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (count at HW2_Part1.java:313)
[INFO ]20161115@21:49:33,305:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
[INFO ]20161115@21:49:33,305:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:49:33,306:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:289), which has no missing parents
[INFO ]20161115@21:49:33,309:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 189 ms on localhost (1/1)
[INFO ]20161115@21:49:33,309:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:49:33,329:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.5 KB, free 451.2 KB)
[INFO ]20161115@21:49:33,353:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161115@21:49:33,354:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.0 KB, free 453.3 KB)
[INFO ]20161115@21:49:33,355:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:58019 in memory (size: 2.0 KB, free: 499.9 MB)
[INFO ]20161115@21:49:33,357:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:58019 (size: 2.0 KB, free: 499.9 MB)
[INFO ]20161115@21:49:33,358:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:49:33,359:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:289)
[INFO ]20161115@21:49:33,359:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
[INFO ]20161115@21:49:33,360:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 5, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@21:49:33,360:org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 5)
[INFO ]20161115@21:49:33,378:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@21:49:33,385:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
[INFO ]20161115@21:49:33,452:org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 5). 1124 bytes result sent to driver
[INFO ]20161115@21:49:33,461:org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (count at HW2_Part1.java:313) finished in 0.096 s
[INFO ]20161115@21:49:33,462:org.apache.spark.scheduler.DAGScheduler - Job 4 finished: count at HW2_Part1.java:313, took 0.158642 s
[INFO ]20161115@21:49:33,478:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 5) in 101 ms on localhost (1/1)
[INFO ]20161115@21:49:33,478:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:49:33,571:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@21:49:33,611:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@21:49:33,645:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@21:49:33,648:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@21:49:33,656:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@21:49:33,667:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@21:49:33,692:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@21:49:33,707:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@21:49:33,725:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161115@21:49:33,755:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@21:49:33,757:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-73533e3e-32f9-40af-8bdf-66d6feb9c30c
[INFO ]20161115@21:58:42,272:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@21:58:42,761:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@21:58:43,202:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@21:58:43,203:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@21:58:43,204:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@21:58:43,606:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 47574.
[INFO ]20161115@21:58:44,157:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@21:58:44,224:Remoting - Starting remoting
[INFO ]20161115@21:58:44,530:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:47377]
[INFO ]20161115@21:58:44,532:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:47377]
[INFO ]20161115@21:58:44,554:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 47377.
[INFO ]20161115@21:58:44,607:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@21:58:44,650:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@21:58:44,680:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-87d938b0-bdd5-4370-bcb2-db87e80530d5
[INFO ]20161115@21:58:44,718:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@21:58:44,831:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@21:58:45,462:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@21:58:45,467:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@21:58:45,729:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@21:58:45,774:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40223.
[INFO ]20161115@21:58:45,775:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 40223
[INFO ]20161115@21:58:45,776:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@21:58:45,783:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:40223 with 500.1 MB RAM, BlockManagerId(driver, localhost, 40223)
[INFO ]20161115@21:58:45,789:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@21:58:47,259:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@21:58:47,681:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@21:58:47,685:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:40223 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@21:58:47,700:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part1.java:119
[INFO ]20161115@21:58:48,278:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@21:58:48,346:org.apache.spark.SparkContext - Starting job: first at HW2_Part1.java:132
[INFO ]20161115@21:58:48,407:org.apache.spark.scheduler.DAGScheduler - Got job 0 (first at HW2_Part1.java:132) with 1 output partitions
[INFO ]20161115@21:58:48,407:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (first at HW2_Part1.java:132)
[INFO ]20161115@21:58:48,408:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
[INFO ]20161115@21:58:48,418:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@21:58:48,553:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:119), which has no missing parents
[INFO ]20161115@21:58:48,649:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 156.5 KB)
[INFO ]20161115@21:58:48,677:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1859.0 B, free 158.3 KB)
[INFO ]20161115@21:58:48,679:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:40223 (size: 1859.0 B, free: 500.1 MB)
[INFO ]20161115@21:58:48,680:org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:58:48,685:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (data/companies/SP500-constituents-financials.csv MapPartitionsRDD[1] at textFile at HW2_Part1.java:119)
[INFO ]20161115@21:58:48,689:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@21:58:48,760:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2196 bytes)
[INFO ]20161115@21:58:48,789:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@21:58:48,883:org.apache.spark.CacheManager - Partition rdd_1_0 not found, computing it
[INFO ]20161115@21:58:48,894:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@21:58:48,972:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@21:58:48,973:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@21:58:48,973:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@21:58:48,973:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@21:58:48,973:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@21:58:49,244:org.apache.spark.storage.MemoryStore - Block rdd_1_0 stored as values in memory (estimated size 181.8 KB, free 340.1 KB)
[INFO ]20161115@21:58:49,252:org.apache.spark.storage.BlockManagerInfo - Added rdd_1_0 in memory on localhost:40223 (size: 181.8 KB, free: 499.9 MB)
[INFO ]20161115@21:58:49,440:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2763 bytes result sent to driver
[INFO ]20161115@21:58:49,621:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 845 ms on localhost (1/1)
[INFO ]20161115@21:58:49,633:org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (first at HW2_Part1.java:132) finished in 0.922 s
[INFO ]20161115@21:58:49,635:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@21:58:49,747:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: first at HW2_Part1.java:132, took 1.396174 s
[INFO ]20161115@21:59:55,230:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161115@21:59:55,460:org.apache.spark.SparkContext - Starting job: saveAsTextFile at HW2_Part1.java:229
[INFO ]20161115@21:59:55,474:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (sortBy at HW2_Part1.java:215)
[INFO ]20161115@21:59:55,477:org.apache.spark.scheduler.DAGScheduler - Got job 1 (saveAsTextFile at HW2_Part1.java:229) with 1 output partitions
[INFO ]20161115@21:59:55,477:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (saveAsTextFile at HW2_Part1.java:229)
[INFO ]20161115@21:59:55,478:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
[INFO ]20161115@21:59:55,478:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 1)
[INFO ]20161115@21:59:55,487:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:215), which has no missing parents
[INFO ]20161115@21:59:55,511:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 5.5 KB, free 345.6 KB)
[INFO ]20161115@21:59:55,533:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 348.7 KB)
[INFO ]20161115@21:59:55,534:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:40223 (size: 3.0 KB, free: 499.9 MB)
[INFO ]20161115@21:59:55,537:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@21:59:55,539:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[4] at sortBy at HW2_Part1.java:215)
[INFO ]20161115@21:59:55,539:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@21:59:55,556:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161115@21:59:55,561:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@21:59:55,625:org.apache.spark.storage.BlockManager - Found block rdd_1_0 locally
[ERROR]20161115@21:59:55,629:homework2.HW2_Part1 - NumberFormatException:
[INFO ]20161115@22:00:08,876:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161115@22:00:08,957:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 13411 ms on localhost (1/1)
[INFO ]20161115@22:00:08,958:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@22:00:08,970:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (sortBy at HW2_Part1.java:215) finished in 13.427 s
[INFO ]20161115@22:00:08,980:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161115@22:00:08,980:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161115@22:00:08,981:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161115@22:00:08,981:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161115@22:00:08,987:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:229), which has no missing parents
[INFO ]20161115@22:00:09,095:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 71.4 KB, free 420.0 KB)
[INFO ]20161115@22:00:09,131:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.0 KB, free 445.0 KB)
[INFO ]20161115@22:00:09,140:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:40223 (size: 25.0 KB, free: 499.9 MB)
[INFO ]20161115@22:00:09,149:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@22:00:09,150:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at saveAsTextFile at HW2_Part1.java:229)
[INFO ]20161115@22:00:09,150:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161115@22:00:09,177:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@22:00:09,179:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161115@22:00:09,777:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@22:00:09,844:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 70 ms
[INFO ]20161115@22:00:09,846:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:40223 in memory (size: 3.0 KB, free: 499.9 MB)
[INFO ]20161115@22:00:10,167:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
[INFO ]20161115@22:00:10,453:org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_201611152159_0002_m_000000_2' to file:/home/cloudera/git/siming.meng/homework2/output/HW2Part1_1479275921575/_temporary/0/task_201611152159_0002_m_000000
[INFO ]20161115@22:00:10,454:org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_201611152159_0002_m_000000_2: Committed
[INFO ]20161115@22:00:10,489:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2038 bytes result sent to driver
[INFO ]20161115@22:00:10,513:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 1359 ms on localhost (1/1)
[INFO ]20161115@22:00:10,513:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161115@22:00:10,538:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (saveAsTextFile at HW2_Part1.java:229) finished in 1.385 s
[INFO ]20161115@22:00:10,541:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: saveAsTextFile at HW2_Part1.java:229, took 15.080073 s
[INFO ]20161115@22:00:12,777:org.apache.spark.ContextCleaner - Cleaned accumulator 2
[INFO ]20161115@22:00:12,778:org.apache.spark.ContextCleaner - Cleaned accumulator 3
[INFO ]20161115@22:00:12,779:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:40223 in memory (size: 25.0 KB, free: 499.9 MB)
[INFO ]20161115@22:00:28,912:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:238
[INFO ]20161115@22:00:28,959:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161115@22:00:28,963:org.apache.spark.scheduler.DAGScheduler - Got job 2 (count at HW2_Part1.java:238) with 1 output partitions
[INFO ]20161115@22:00:28,964:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (count at HW2_Part1.java:238)
[INFO ]20161115@22:00:28,964:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3)
[INFO ]20161115@22:00:28,964:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@22:00:28,966:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:215), which has no missing parents
[INFO ]20161115@22:00:28,990:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 343.6 KB)
[INFO ]20161115@22:00:29,005:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 345.6 KB)
[INFO ]20161115@22:00:29,016:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:40223 (size: 2.0 KB, free: 499.9 MB)
[INFO ]20161115@22:00:29,017:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@22:00:29,018:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:215)
[INFO ]20161115@22:00:29,018:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
[INFO ]20161115@22:00:29,021:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 3, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@22:00:29,023:org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 3)
[INFO ]20161115@22:00:29,049:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@22:00:29,054:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
[INFO ]20161115@22:00:29,163:org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 3). 1124 bytes result sent to driver
[INFO ]20161115@22:00:29,202:org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (count at HW2_Part1.java:238) finished in 0.167 s
[INFO ]20161115@22:00:29,203:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: count at HW2_Part1.java:238, took 0.289470 s
[INFO ]20161115@22:00:29,210:homework2.HW2_Part1 - Valid records:  504
[INFO ]20161115@22:00:29,215:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 3) in 176 ms on localhost (1/1)
[INFO ]20161115@22:00:29,215:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO ]20161115@22:00:30,578:org.apache.spark.SparkContext - Starting job: count at HW2_Part1.java:239
[INFO ]20161115@22:00:30,586:org.apache.spark.scheduler.DAGScheduler - Got job 3 (count at HW2_Part1.java:239) with 1 output partitions
[INFO ]20161115@22:00:30,587:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (count at HW2_Part1.java:239)
[INFO ]20161115@22:00:30,587:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
[INFO ]20161115@22:00:30,587:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@22:00:30,588:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:215), which has no missing parents
[INFO ]20161115@22:00:30,592:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.5 KB, free 349.1 KB)
[INFO ]20161115@22:00:30,625:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161115@22:00:30,627:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:40223 in memory (size: 2.0 KB, free: 499.9 MB)
[INFO ]20161115@22:00:30,652:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.0 KB, free 345.6 KB)
[INFO ]20161115@22:00:30,654:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:40223 (size: 2.0 KB, free: 499.9 MB)
[INFO ]20161115@22:00:30,656:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@22:00:30,658:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[6] at sortBy at HW2_Part1.java:215)
[INFO ]20161115@22:00:30,668:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
[INFO ]20161115@22:00:30,670:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 4, localhost, partition 0,NODE_LOCAL, 1894 bytes)
[INFO ]20161115@22:00:30,670:org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 4)
[INFO ]20161115@22:00:30,692:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@22:00:30,696:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161115@22:00:30,766:org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 4). 1124 bytes result sent to driver
[INFO ]20161115@22:00:30,772:org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (count at HW2_Part1.java:239) finished in 0.093 s
[INFO ]20161115@22:00:30,773:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: count at HW2_Part1.java:239, took 0.187821 s
[INFO ]20161115@22:00:30,777:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 4) in 102 ms on localhost (1/1)
[INFO ]20161115@22:00:30,778:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO ]20161115@22:00:30,855:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@22:00:30,913:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@22:00:30,949:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@22:00:30,959:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@22:00:30,965:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@22:00:30,977:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@22:00:31,016:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@22:00:31,021:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@22:00:31,045:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161115@22:00:31,133:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@22:00:31,134:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-097c0fce-b1f5-4f6b-811c-516ed7b39c05
[INFO ]20161115@23:11:11,847:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@23:11:15,816:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@23:11:18,167:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@23:11:18,427:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@23:11:18,430:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@23:11:20,044:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45763.
[INFO ]20161115@23:11:22,491:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@23:11:23,216:Remoting - Starting remoting
[INFO ]20161115@23:11:25,278:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:59349]
[INFO ]20161115@23:11:25,446:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:59349]
[INFO ]20161115@23:11:26,021:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 59349.
[INFO ]20161115@23:11:26,147:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@23:11:26,927:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@23:11:27,232:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-53a46c0f-d00e-4281-bae1-185556fd7212
[INFO ]20161115@23:11:27,394:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@23:11:28,342:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@23:11:30,270:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@23:11:30,277:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@23:11:31,096:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@23:11:31,237:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36814.
[INFO ]20161115@23:11:31,238:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 36814
[INFO ]20161115@23:11:31,241:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@23:11:31,247:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:36814 with 500.1 MB RAM, BlockManagerId(driver, localhost, 36814)
[INFO ]20161115@23:11:31,269:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@23:12:08,479:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@23:12:11,076:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@23:12:11,093:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:36814 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@23:12:11,138:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:77
[INFO ]20161115@23:12:15,742:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161115@23:12:16,183:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161115@23:12:16,289:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:36814 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@23:12:16,512:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:78
[INFO ]20161115@23:12:58,082:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@23:13:00,323:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@23:13:07,955:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:87
[INFO ]20161115@23:13:08,184:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:79)
[INFO ]20161115@23:13:08,184:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:80)
[INFO ]20161115@23:13:08,186:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:87) with 1 output partitions
[INFO ]20161115@23:13:08,187:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:87)
[INFO ]20161115@23:13:08,187:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161115@23:13:08,224:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161115@23:13:08,364:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:79), which has no missing parents
[INFO ]20161115@23:13:08,816:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161115@23:13:09,291:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161115@23:13:09,292:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:36814 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161115@23:13:09,294:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:13:09,308:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:79)
[INFO ]20161115@23:13:09,315:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@23:13:09,350:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:80), which has no missing parents
[INFO ]20161115@23:13:09,354:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 316.8 KB)
[INFO ]20161115@23:13:09,388:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161115@23:13:09,777:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161115@23:13:09,803:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:36814 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161115@23:13:09,804:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:13:09,804:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:80)
[INFO ]20161115@23:13:09,805:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@23:13:09,860:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@23:13:10,204:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@23:13:10,252:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@23:13:10,252:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@23:13:10,253:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@23:13:10,253:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@23:13:10,253:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@23:13:11,635:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161115@23:13:11,735:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2173 bytes)
[INFO ]20161115@23:13:11,743:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@23:13:11,867:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161115@23:13:12,209:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 2455 ms on localhost (1/1)
[INFO ]20161115@23:13:12,243:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:13:12,263:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:79) finished in 2.906 s
[INFO ]20161115@23:13:12,473:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161115@23:13:12,474:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161115@23:13:12,475:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161115@23:13:12,478:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161115@23:13:12,749:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161115@23:13:13,109:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1285 ms on localhost (1/1)
[INFO ]20161115@23:13:13,110:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:13:13,116:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:80) finished in 3.303 s
[INFO ]20161115@23:13:13,116:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161115@23:13:13,118:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161115@23:13:13,123:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161115@23:13:13,124:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161115@23:13:13,129:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:82), which has no missing parents
[INFO ]20161115@23:13:13,317:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 322.4 KB)
[INFO ]20161115@23:13:14,193:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1914.0 B, free 324.2 KB)
[INFO ]20161115@23:13:14,202:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:36814 (size: 1914.0 B, free: 500.1 MB)
[INFO ]20161115@23:13:14,227:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:13:14,303:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:82)
[INFO ]20161115@23:13:14,303:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161115@23:13:14,318:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161115@23:13:14,320:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161115@23:13:14,345:org.apache.spark.CacheManager - Partition rdd_8_0 not found, computing it
[INFO ]20161115@23:13:14,443:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@23:13:14,447:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 9 ms
[INFO ]20161115@23:13:14,597:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@23:13:14,608:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 10 ms
[INFO ]20161115@23:13:16,371:org.apache.spark.storage.MemoryStore - Block rdd_8_0 stored as values in memory (estimated size 198.3 KB, free 522.6 KB)
[INFO ]20161115@23:13:16,374:org.apache.spark.storage.BlockManagerInfo - Added rdd_8_0 in memory on localhost:36814 (size: 198.3 KB, free: 499.9 MB)
[INFO ]20161115@23:13:16,402:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1704 bytes result sent to driver
[INFO ]20161115@23:13:16,555:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 2243 ms on localhost (1/1)
[INFO ]20161115@23:13:16,555:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:13:16,715:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:87) finished in 2.408 s
[INFO ]20161115@23:13:17,024:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:87, took 9.025599 s
[INFO ]20161115@23:14:00,327:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:97
[INFO ]20161115@23:14:00,389:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161115@23:14:00,395:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161115@23:14:00,395:org.apache.spark.scheduler.DAGScheduler - Got job 1 (takeOrdered at HW2_Part2.java:97) with 1 output partitions
[INFO ]20161115@23:14:00,396:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (takeOrdered at HW2_Part2.java:97)
[INFO ]20161115@23:14:00,396:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161115@23:14:00,398:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@23:14:00,400:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:97), which has no missing parents
[INFO ]20161115@23:14:00,415:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 4.9 KB, free 527.4 KB)
[INFO ]20161115@23:14:00,450:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.6 KB, free 530.1 KB)
[INFO ]20161115@23:14:00,555:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:36814 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161115@23:14:00,557:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:14:00,557:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:97)
[INFO ]20161115@23:14:00,557:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161115@23:14:00,984:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161115@23:14:00,985:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161115@23:14:01,142:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161115@23:14:01,170:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 2261 bytes result sent to driver
[INFO ]20161115@23:14:01,502:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (takeOrdered at HW2_Part2.java:97) finished in 0.525 s
[INFO ]20161115@23:14:01,506:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: takeOrdered at HW2_Part2.java:97, took 1.155170 s
[INFO ]20161115@23:14:01,509:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 530 ms on localhost (1/1)
[INFO ]20161115@23:14:01,509:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:14:21,381:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161115@23:14:21,526:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@23:14:21,710:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@23:14:21,984:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@23:14:21,986:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@23:14:21,991:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@23:14:21,996:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@23:14:22,041:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@23:14:22,042:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@23:14:22,043:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-87cd45ac-1d8d-463a-a9d2-d3af42bb28bc
[INFO ]20161115@23:34:42,640:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@23:34:48,265:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@23:34:51,111:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@23:34:51,119:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@23:34:51,119:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@23:34:53,397:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39665.
[INFO ]20161115@23:34:56,936:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@23:34:57,274:Remoting - Starting remoting
[INFO ]20161115@23:34:59,012:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:46087]
[INFO ]20161115@23:34:59,022:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:46087]
[INFO ]20161115@23:34:59,822:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 46087.
[INFO ]20161115@23:35:00,090:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@23:35:00,381:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@23:35:00,803:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-2daea4fc-5adb-4723-b779-99c43f418b32
[INFO ]20161115@23:35:00,955:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@23:35:01,875:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@23:35:04,427:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@23:35:04,483:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@23:35:06,636:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@23:35:07,131:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47333.
[INFO ]20161115@23:35:07,133:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 47333
[INFO ]20161115@23:35:07,140:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@23:35:07,146:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:47333 with 500.1 MB RAM, BlockManagerId(driver, localhost, 47333)
[INFO ]20161115@23:35:07,149:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@23:35:22,225:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@23:35:23,511:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@23:35:23,829:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:47333 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@23:35:23,917:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:77
[INFO ]20161115@23:35:24,439:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161115@23:35:24,599:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161115@23:35:24,603:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:47333 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@23:35:24,608:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:78
[INFO ]20161115@23:35:26,341:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@23:35:26,510:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@23:35:26,595:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:88
[INFO ]20161115@23:35:26,937:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:79)
[INFO ]20161115@23:35:26,940:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:80)
[INFO ]20161115@23:35:26,954:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:88) with 1 output partitions
[INFO ]20161115@23:35:26,955:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:88)
[INFO ]20161115@23:35:26,957:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161115@23:35:26,970:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161115@23:35:27,021:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:79), which has no missing parents
[INFO ]20161115@23:35:27,432:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161115@23:35:27,783:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161115@23:35:27,784:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:47333 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161115@23:35:27,787:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:35:27,828:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:79)
[INFO ]20161115@23:35:27,855:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@23:35:28,004:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:80), which has no missing parents
[INFO ]20161115@23:35:28,024:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 316.8 KB)
[INFO ]20161115@23:35:28,100:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161115@23:35:28,267:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161115@23:35:28,285:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:47333 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161115@23:35:28,286:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:35:28,286:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:80)
[INFO ]20161115@23:35:28,286:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@23:35:28,583:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@23:35:28,830:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@23:35:28,913:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@23:35:28,915:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@23:35:28,915:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@23:35:28,915:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@23:35:28,915:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@23:35:30,900:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161115@23:35:31,053:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2173 bytes)
[INFO ]20161115@23:35:31,055:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@23:35:31,180:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161115@23:35:32,297:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 4041 ms on localhost (1/1)
[INFO ]20161115@23:35:32,431:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161115@23:35:32,647:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:35:32,686:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:79) finished in 4.686 s
[INFO ]20161115@23:35:32,735:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1662 ms on localhost (1/1)
[INFO ]20161115@23:35:32,736:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:35:32,859:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161115@23:35:32,878:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161115@23:35:32,878:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161115@23:35:32,879:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161115@23:35:32,891:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:80) finished in 4.603 s
[INFO ]20161115@23:35:32,896:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161115@23:35:32,898:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161115@23:35:32,899:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161115@23:35:32,899:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161115@23:35:32,899:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:82), which has no missing parents
[INFO ]20161115@23:35:33,114:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 322.4 KB)
[INFO ]20161115@23:35:33,192:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1914.0 B, free 324.2 KB)
[INFO ]20161115@23:35:33,193:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:47333 (size: 1914.0 B, free: 500.1 MB)
[INFO ]20161115@23:35:33,201:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:35:33,206:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:82)
[INFO ]20161115@23:35:33,207:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161115@23:35:33,210:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161115@23:35:33,211:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161115@23:35:33,229:org.apache.spark.CacheManager - Partition rdd_8_0 not found, computing it
[INFO ]20161115@23:35:33,249:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@23:35:33,303:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 61 ms
[INFO ]20161115@23:35:33,739:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@23:35:33,740:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161115@23:35:37,854:org.apache.spark.storage.MemoryStore - Block rdd_8_0 stored as values in memory (estimated size 198.3 KB, free 522.6 KB)
[INFO ]20161115@23:35:37,860:org.apache.spark.storage.BlockManagerInfo - Added rdd_8_0 in memory on localhost:47333 (size: 198.3 KB, free: 499.9 MB)
[INFO ]20161115@23:35:37,902:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1704 bytes result sent to driver
[INFO ]20161115@23:35:38,055:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 4834 ms on localhost (1/1)
[INFO ]20161115@23:35:38,061:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:35:38,172:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:88) finished in 4.960 s
[INFO ]20161115@23:35:38,653:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:88, took 12.057607 s
[INFO ]20161115@23:35:47,639:org.apache.spark.SparkContext - Starting job: collect at HW2_Part2.java:91
[INFO ]20161115@23:35:47,736:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161115@23:35:47,756:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161115@23:35:47,757:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part2.java:91) with 1 output partitions
[INFO ]20161115@23:35:47,761:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at HW2_Part2.java:91)
[INFO ]20161115@23:35:47,767:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161115@23:35:47,797:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@23:35:47,798:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:82), which has no missing parents
[INFO ]20161115@23:35:47,811:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 526.0 KB)
[INFO ]20161115@23:35:47,870:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1950.0 B, free 527.9 KB)
[INFO ]20161115@23:35:47,873:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:47333 (size: 1950.0 B, free: 499.9 MB)
[INFO ]20161115@23:35:47,874:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:35:47,876:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:82)
[INFO ]20161115@23:35:47,880:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161115@23:35:48,117:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161115@23:35:48,126:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161115@23:35:48,169:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161115@23:35:48,244:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 35565 bytes result sent to driver
[INFO ]20161115@23:35:48,338:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at HW2_Part2.java:91) finished in 0.282 s
[INFO ]20161115@23:35:48,339:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at HW2_Part2.java:91, took 0.695635 s
[INFO ]20161115@23:35:48,340:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 282 ms on localhost (1/1)
[INFO ]20161115@23:35:48,340:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:36:54,952:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161115@23:36:55,245:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:103
[INFO ]20161115@23:36:55,326:org.apache.spark.scheduler.DAGScheduler - Got job 2 (takeOrdered at HW2_Part2.java:103) with 1 output partitions
[INFO ]20161115@23:36:55,328:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (takeOrdered at HW2_Part2.java:103)
[INFO ]20161115@23:36:55,328:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161115@23:36:55,339:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@23:36:55,344:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:103), which has no missing parents
[INFO ]20161115@23:36:55,348:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.9 KB, free 530.8 KB)
[INFO ]20161115@23:36:55,352:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:47333 in memory (size: 1950.0 B, free: 499.9 MB)
[INFO ]20161115@23:36:55,426:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 530.1 KB)
[INFO ]20161115@23:36:55,435:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:47333 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161115@23:36:55,438:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:36:55,439:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:103)
[INFO ]20161115@23:36:55,439:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161115@23:36:55,443:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161115@23:36:55,511:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 4)
[INFO ]20161115@23:36:55,585:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161115@23:36:55,687:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 4). 2261 bytes result sent to driver
[INFO ]20161115@23:36:55,732:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (takeOrdered at HW2_Part2.java:103) finished in 0.289 s
[INFO ]20161115@23:36:55,734:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: takeOrdered at HW2_Part2.java:103, took 0.485238 s
[INFO ]20161115@23:36:55,736:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 4) in 289 ms on localhost (1/1)
[INFO ]20161115@23:36:55,748:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:36:55,858:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161115@23:36:56,229:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@23:36:56,296:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@23:36:56,351:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@23:36:56,354:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@23:36:56,361:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@23:36:56,372:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@23:36:56,438:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@23:36:56,440:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@23:36:56,444:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-a78bbf41-f87b-40dc-ab5f-77b11687d9dd
[INFO ]20161115@23:39:18,512:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161115@23:39:21,998:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161115@23:39:24,942:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161115@23:39:24,943:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161115@23:39:24,972:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161115@23:39:28,403:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 48195.
[INFO ]20161115@23:39:30,484:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161115@23:39:31,520:Remoting - Starting remoting
[INFO ]20161115@23:39:34,371:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:48388]
[INFO ]20161115@23:39:34,504:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:48388]
[INFO ]20161115@23:39:34,533:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 48388.
[INFO ]20161115@23:39:34,787:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161115@23:39:35,421:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161115@23:39:35,516:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-62355dc0-d3eb-410b-a42f-e7854306401a
[INFO ]20161115@23:39:35,540:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161115@23:39:36,392:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161115@23:39:40,016:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161115@23:39:40,021:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161115@23:39:42,121:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161115@23:39:42,340:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52433.
[INFO ]20161115@23:39:42,348:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 52433
[INFO ]20161115@23:39:42,442:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161115@23:39:42,505:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:52433 with 500.1 MB RAM, BlockManagerId(driver, localhost, 52433)
[INFO ]20161115@23:39:42,520:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161115@23:40:41,128:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161115@23:40:43,343:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161115@23:40:43,520:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:52433 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@23:40:43,600:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:77
[INFO ]20161115@23:40:43,970:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161115@23:40:44,216:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161115@23:40:44,216:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:52433 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161115@23:40:44,229:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:78
[INFO ]20161115@23:40:46,024:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@23:40:46,685:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161115@23:40:47,087:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:88
[INFO ]20161115@23:40:47,290:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:79)
[INFO ]20161115@23:40:47,292:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:80)
[INFO ]20161115@23:40:47,298:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:88) with 1 output partitions
[INFO ]20161115@23:40:47,299:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:88)
[INFO ]20161115@23:40:47,300:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161115@23:40:47,324:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161115@23:40:47,351:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:79), which has no missing parents
[INFO ]20161115@23:40:47,488:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161115@23:40:47,733:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161115@23:40:47,751:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:52433 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161115@23:40:47,758:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:40:47,847:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:79)
[INFO ]20161115@23:40:47,850:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161115@23:40:47,869:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:80), which has no missing parents
[INFO ]20161115@23:40:47,925:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 316.8 KB)
[INFO ]20161115@23:40:47,986:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161115@23:40:48,631:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161115@23:40:48,657:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:52433 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161115@23:40:48,740:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:40:48,741:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:80)
[INFO ]20161115@23:40:48,743:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161115@23:40:48,744:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161115@23:40:50,172:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161115@23:40:50,211:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161115@23:40:50,211:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161115@23:40:50,211:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161115@23:40:50,212:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161115@23:40:50,212:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161115@23:40:53,494:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161115@23:40:54,530:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2173 bytes)
[INFO ]20161115@23:40:54,547:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161115@23:40:54,753:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161115@23:40:55,185:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 6955 ms on localhost (1/1)
[INFO ]20161115@23:40:55,450:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:79) finished in 7.315 s
[INFO ]20161115@23:40:55,450:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:40:55,457:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161115@23:40:55,470:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161115@23:40:55,470:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161115@23:40:55,471:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161115@23:40:55,519:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161115@23:40:55,576:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:80) finished in 6.832 s
[INFO ]20161115@23:40:55,576:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161115@23:40:55,576:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161115@23:40:55,576:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161115@23:40:55,578:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161115@23:40:55,579:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:82), which has no missing parents
[INFO ]20161115@23:40:55,581:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1293 ms on localhost (1/1)
[INFO ]20161115@23:40:55,582:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:40:55,609:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 322.4 KB)
[INFO ]20161115@23:40:55,642:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1912.0 B, free 324.2 KB)
[INFO ]20161115@23:40:55,647:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:52433 (size: 1912.0 B, free: 500.1 MB)
[INFO ]20161115@23:40:55,699:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:40:55,794:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:82)
[INFO ]20161115@23:40:55,794:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161115@23:40:55,798:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161115@23:40:55,799:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161115@23:40:55,965:org.apache.spark.CacheManager - Partition rdd_8_0 not found, computing it
[INFO ]20161115@23:40:56,133:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@23:40:56,144:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 14 ms
[INFO ]20161115@23:40:56,327:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161115@23:40:56,335:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 9 ms
[INFO ]20161115@23:40:59,473:org.apache.spark.storage.MemoryStore - Block rdd_8_0 stored as values in memory (estimated size 198.3 KB, free 522.6 KB)
[INFO ]20161115@23:40:59,519:org.apache.spark.storage.BlockManagerInfo - Added rdd_8_0 in memory on localhost:52433 (size: 198.3 KB, free: 499.9 MB)
[INFO ]20161115@23:40:59,770:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1704 bytes result sent to driver
[INFO ]20161115@23:41:00,048:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 4250 ms on localhost (1/1)
[INFO ]20161115@23:41:00,049:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:41:00,175:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:88) finished in 4.370 s
[INFO ]20161115@23:41:00,579:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:88, took 13.491135 s
[INFO ]20161115@23:41:11,193:org.apache.spark.SparkContext - Starting job: collect at HW2_Part2.java:91
[INFO ]20161115@23:41:11,242:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161115@23:41:11,250:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161115@23:41:11,250:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part2.java:91) with 1 output partitions
[INFO ]20161115@23:41:11,251:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at HW2_Part2.java:91)
[INFO ]20161115@23:41:11,253:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161115@23:41:11,329:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@23:41:11,331:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:82), which has no missing parents
[INFO ]20161115@23:41:11,332:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 526.0 KB)
[INFO ]20161115@23:41:11,344:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1948.0 B, free 527.9 KB)
[INFO ]20161115@23:41:11,346:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:52433 (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161115@23:41:11,346:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:41:11,347:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:82)
[INFO ]20161115@23:41:11,347:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161115@23:41:11,477:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161115@23:41:11,478:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161115@23:41:11,508:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161115@23:41:11,518:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 35565 bytes result sent to driver
[INFO ]20161115@23:41:11,711:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at HW2_Part2.java:91) finished in 0.360 s
[INFO ]20161115@23:41:11,712:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at HW2_Part2.java:91, took 0.512295 s
[INFO ]20161115@23:41:11,714:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 360 ms on localhost (1/1)
[INFO ]20161115@23:41:11,714:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:41:12,545:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161115@23:41:12,854:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:104
[INFO ]20161115@23:41:12,935:org.apache.spark.scheduler.DAGScheduler - Got job 2 (takeOrdered at HW2_Part2.java:104) with 1 output partitions
[INFO ]20161115@23:41:12,936:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (takeOrdered at HW2_Part2.java:104)
[INFO ]20161115@23:41:12,936:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161115@23:41:12,943:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161115@23:41:12,943:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:104), which has no missing parents
[INFO ]20161115@23:41:12,956:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.9 KB, free 532.7 KB)
[INFO ]20161115@23:41:12,970:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 535.4 KB)
[INFO ]20161115@23:41:12,971:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:52433 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161115@23:41:12,972:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161115@23:41:12,972:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:104)
[INFO ]20161115@23:41:12,972:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161115@23:41:12,973:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161115@23:41:12,984:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 4)
[INFO ]20161115@23:41:13,060:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:52433 in memory (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161115@23:41:13,071:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161115@23:41:13,484:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 4). 2261 bytes result sent to driver
[INFO ]20161115@23:41:13,641:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (takeOrdered at HW2_Part2.java:104) finished in 0.668 s
[INFO ]20161115@23:41:13,643:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: takeOrdered at HW2_Part2.java:104, took 0.782925 s
[INFO ]20161115@23:41:13,644:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 4) in 669 ms on localhost (1/1)
[INFO ]20161115@23:41:13,644:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161115@23:41:14,266:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161115@23:41:14,517:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161115@23:41:15,313:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161115@23:41:15,705:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161115@23:41:15,706:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161115@23:41:15,729:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161115@23:41:15,759:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161115@23:41:15,820:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161115@23:41:15,821:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-f0a415c6-f17d-4c75-b7c7-c98817756b56
[INFO ]20161115@23:41:15,821:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161115@23:41:15,832:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161115@23:41:15,845:akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[INFO ]20161116@11:02:06,143:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@11:02:08,688:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@11:02:11,099:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@11:02:11,100:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@11:02:11,100:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@11:02:12,227:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 49331.
[INFO ]20161116@11:02:14,192:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@11:02:14,456:Remoting - Starting remoting
[INFO ]20161116@11:02:15,182:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:53214]
[INFO ]20161116@11:02:15,219:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:53214]
[INFO ]20161116@11:02:15,209:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 53214.
[INFO ]20161116@11:02:15,373:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@11:02:15,406:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@11:02:15,436:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-e0bcf605-2f05-43ed-ba3a-e84cc6994081
[INFO ]20161116@11:02:15,459:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161116@11:02:15,650:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@11:02:16,490:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@11:02:16,517:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@11:02:17,087:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@11:02:17,153:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59405.
[INFO ]20161116@11:02:17,157:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 59405
[INFO ]20161116@11:02:17,161:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@11:02:17,247:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:59405 with 500.1 MB RAM, BlockManagerId(driver, localhost, 59405)
[INFO ]20161116@11:02:17,490:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@11:02:19,896:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@11:02:20,723:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@11:02:20,752:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:59405 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@11:02:20,858:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:91
[INFO ]20161116@11:02:21,007:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@11:02:21,033:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@11:02:21,042:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:59405 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@11:02:21,043:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:92
[INFO ]20161116@11:02:22,505:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@11:02:22,875:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@11:02:23,654:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:102
[INFO ]20161116@11:02:23,820:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:93)
[INFO ]20161116@11:02:23,822:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:94)
[INFO ]20161116@11:02:23,826:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:102) with 1 output partitions
[INFO ]20161116@11:02:23,826:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:102)
[INFO ]20161116@11:02:23,827:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@11:02:23,834:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@11:02:23,862:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:93), which has no missing parents
[INFO ]20161116@11:02:24,056:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161116@11:02:24,240:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161116@11:02:24,241:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:59405 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@11:02:24,242:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@11:02:24,278:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:93)
[INFO ]20161116@11:02:24,286:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@11:02:24,367:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:94), which has no missing parents
[INFO ]20161116@11:02:24,372:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 316.8 KB)
[INFO ]20161116@11:02:24,390:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161116@11:02:24,607:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161116@11:02:24,617:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:59405 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@11:02:24,618:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@11:02:24,618:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:94)
[INFO ]20161116@11:02:24,619:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@11:02:24,687:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@11:02:24,732:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@11:02:24,810:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@11:02:24,811:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@11:02:24,811:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@11:02:24,811:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@11:02:24,812:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@11:02:25,712:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161116@11:02:25,719:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2173 bytes)
[INFO ]20161116@11:02:25,720:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@11:02:25,757:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@11:02:26,344:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1806 ms on localhost (1/1)
[INFO ]20161116@11:02:26,369:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@11:02:26,390:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:93) finished in 2.011 s
[INFO ]20161116@11:02:26,391:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@11:02:26,395:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161116@11:02:26,395:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@11:02:26,395:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@11:02:26,497:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161116@11:02:26,670:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:94) finished in 2.051 s
[INFO ]20161116@11:02:26,670:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@11:02:26,671:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@11:02:26,671:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@11:02:26,671:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@11:02:26,672:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:96), which has no missing parents
[INFO ]20161116@11:02:26,673:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 951 ms on localhost (1/1)
[INFO ]20161116@11:02:26,673:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@11:02:26,715:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 322.4 KB)
[INFO ]20161116@11:02:26,969:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1912.0 B, free 324.2 KB)
[INFO ]20161116@11:02:26,971:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:59405 (size: 1912.0 B, free: 500.1 MB)
[INFO ]20161116@11:02:26,979:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@11:02:26,988:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:96)
[INFO ]20161116@11:02:26,988:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@11:02:26,991:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@11:02:26,991:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@11:02:27,003:org.apache.spark.CacheManager - Partition rdd_8_0 not found, computing it
[INFO ]20161116@11:02:27,013:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@11:02:27,018:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
[INFO ]20161116@11:02:27,190:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@11:02:27,190:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
[INFO ]20161116@11:02:28,540:org.apache.spark.storage.MemoryStore - Block rdd_8_0 stored as values in memory (estimated size 198.3 KB, free 522.6 KB)
[INFO ]20161116@11:02:28,542:org.apache.spark.storage.BlockManagerInfo - Added rdd_8_0 in memory on localhost:59405 (size: 198.3 KB, free: 499.9 MB)
[INFO ]20161116@11:02:28,569:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1704 bytes result sent to driver
[INFO ]20161116@11:02:29,148:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 2158 ms on localhost (1/1)
[INFO ]20161116@11:02:29,148:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@11:02:29,435:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:102) finished in 2.443 s
[INFO ]20161116@11:02:29,649:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:102, took 5.993965 s
[INFO ]20161116@11:02:30,113:org.apache.spark.SparkContext - Starting job: collect at HW2_Part2.java:105
[INFO ]20161116@11:02:30,148:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161116@11:02:30,154:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161116@11:02:30,158:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part2.java:105) with 1 output partitions
[INFO ]20161116@11:02:30,159:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at HW2_Part2.java:105)
[INFO ]20161116@11:02:30,159:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@11:02:30,163:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@11:02:30,165:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:96), which has no missing parents
[INFO ]20161116@11:02:30,169:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 526.0 KB)
[INFO ]20161116@11:02:30,176:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1948.0 B, free 527.9 KB)
[INFO ]20161116@11:02:30,177:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:59405 (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161116@11:02:30,178:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@11:02:30,178:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:96)
[INFO ]20161116@11:02:30,179:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@11:02:30,226:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@11:02:30,228:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161116@11:02:30,248:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@11:02:30,274:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 35565 bytes result sent to driver
[INFO ]20161116@11:02:30,334:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at HW2_Part2.java:105) finished in 0.149 s
[INFO ]20161116@11:02:30,335:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at HW2_Part2.java:105, took 0.219250 s
[INFO ]20161116@11:02:30,336:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 149 ms on localhost (1/1)
[INFO ]20161116@11:02:30,336:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@11:02:30,938:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@11:02:30,974:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:118
[INFO ]20161116@11:02:30,978:org.apache.spark.scheduler.DAGScheduler - Got job 2 (takeOrdered at HW2_Part2.java:118) with 1 output partitions
[INFO ]20161116@11:02:30,979:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (takeOrdered at HW2_Part2.java:118)
[INFO ]20161116@11:02:30,979:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161116@11:02:30,980:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@11:02:30,981:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:118), which has no missing parents
[INFO ]20161116@11:02:30,988:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.9 KB, free 532.7 KB)
[INFO ]20161116@11:02:31,011:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 533.5 KB)
[INFO ]20161116@11:02:31,036:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:59405 in memory (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161116@11:02:31,037:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:59405 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@11:02:31,037:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@11:02:31,037:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:118)
[INFO ]20161116@11:02:31,037:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161116@11:02:31,038:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@11:02:31,047:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 4)
[INFO ]20161116@11:02:31,153:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@11:02:31,296:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 4). 2261 bytes result sent to driver
[INFO ]20161116@11:02:31,328:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (takeOrdered at HW2_Part2.java:118) finished in 0.290 s
[INFO ]20161116@11:02:31,330:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: takeOrdered at HW2_Part2.java:118, took 0.353230 s
[INFO ]20161116@11:02:31,337:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 4) in 289 ms on localhost (1/1)
[INFO ]20161116@11:02:31,337:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161116@11:02:55,378:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:135
[INFO ]20161116@11:02:55,382:org.apache.spark.scheduler.DAGScheduler - Got job 3 (takeOrdered at HW2_Part2.java:135) with 1 output partitions
[INFO ]20161116@11:02:55,382:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (takeOrdered at HW2_Part2.java:135)
[INFO ]20161116@11:02:55,382:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9, ShuffleMapStage 10)
[INFO ]20161116@11:02:55,388:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@11:02:55,390:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:135), which has no missing parents
[INFO ]20161116@11:02:55,396:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 534.9 KB)
[INFO ]20161116@11:02:55,570:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 537.6 KB)
[INFO ]20161116@11:02:55,571:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:59405 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@11:02:55,573:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@11:02:55,574:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:135)
[INFO ]20161116@11:02:55,574:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
[INFO ]20161116@11:02:55,576:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@11:02:55,577:org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 5)
[INFO ]20161116@11:02:55,606:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@11:02:55,626:org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 5). 2259 bytes result sent to driver
[INFO ]20161116@11:02:56,052:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@11:02:56,054:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:59405 in memory (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@11:02:56,168:org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (takeOrdered at HW2_Part2.java:135) finished in 0.590 s
[INFO ]20161116@11:02:56,169:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: takeOrdered at HW2_Part2.java:135, took 0.789000 s
[INFO ]20161116@11:02:56,169:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 5) in 592 ms on localhost (1/1)
[INFO ]20161116@11:02:56,185:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ]20161116@11:02:56,281:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@11:02:56,511:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@11:02:56,791:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@11:02:56,900:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@11:02:56,901:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@11:02:56,903:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@11:02:56,906:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@11:02:56,951:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@11:02:56,951:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@11:02:56,952:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-0a3a18eb-a901-41e4-bf89-04543846a084
[ERROR]20161116@12:20:49,671:homework2.HW2_Part2 - Can't create output file
[ERROR]20161116@12:32:09,116:homework2.HW2_Part2 - Can't create output file
[ERROR]20161116@12:33:20,614:homework2.HW2_Part2 - Can't create output file ex[java.io.FileNotFoundException: output/HW2Part2_1479328400493/outcome.txt (No such file or directory)
[ERROR]20161116@12:35:01,484:homework2.HW2_Part2 - Can't create output file ex[java.io.FileNotFoundException: output/HW2Part2_1479328501479/outcome.txt (No such file or directory)
[INFO ]20161116@12:37:05,651:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@12:37:08,502:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@12:37:11,400:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@12:37:11,537:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@12:37:11,539:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@12:37:12,635:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 55493.
[INFO ]20161116@12:37:14,712:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@12:37:15,404:Remoting - Starting remoting
[INFO ]20161116@12:37:17,025:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:32862]
[INFO ]20161116@12:37:17,119:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:32862]
[INFO ]20161116@12:37:17,192:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 32862.
[INFO ]20161116@12:37:17,639:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@12:37:18,087:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@12:37:18,188:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-8a68e0e1-ca9c-44de-b021-58c6405e78e3
[INFO ]20161116@12:37:18,458:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161116@12:37:18,777:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@12:37:20,701:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@12:37:20,703:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@12:37:21,844:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@12:37:22,518:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57124.
[INFO ]20161116@12:37:22,559:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 57124
[INFO ]20161116@12:37:22,565:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@12:37:23,111:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:57124 with 500.1 MB RAM, BlockManagerId(driver, localhost, 57124)
[INFO ]20161116@12:37:23,125:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@12:37:28,442:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@12:37:29,363:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@12:37:29,368:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:57124 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@12:37:29,485:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:110
[INFO ]20161116@12:37:29,866:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@12:37:30,005:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@12:37:30,014:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:57124 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@12:37:30,016:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:111
[INFO ]20161116@12:37:31,887:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@12:37:31,946:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@12:37:32,188:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:121
[INFO ]20161116@12:37:32,446:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:112)
[INFO ]20161116@12:37:32,450:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:113)
[INFO ]20161116@12:37:32,464:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:121) with 1 output partitions
[INFO ]20161116@12:37:32,474:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:121)
[INFO ]20161116@12:37:32,533:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@12:37:32,760:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@12:37:33,466:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:112), which has no missing parents
[INFO ]20161116@12:37:34,109:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161116@12:37:34,225:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161116@12:37:34,226:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:57124 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@12:37:34,227:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:37:34,253:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:112)
[INFO ]20161116@12:37:34,255:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@12:37:34,288:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:113), which has no missing parents
[INFO ]20161116@12:37:34,299:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 316.8 KB)
[INFO ]20161116@12:37:34,351:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161116@12:37:34,419:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161116@12:37:34,428:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:57124 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@12:37:34,429:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:37:34,429:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:113)
[INFO ]20161116@12:37:34,429:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@12:37:34,539:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@12:37:35,140:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@12:37:35,163:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@12:37:35,163:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@12:37:35,164:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@12:37:35,164:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@12:37:35,164:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@12:37:35,603:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161116@12:37:35,609:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2173 bytes)
[INFO ]20161116@12:37:35,610:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@12:37:35,665:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@12:37:35,970:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161116@12:37:36,006:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1509 ms on localhost (1/1)
[INFO ]20161116@12:37:36,008:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:37:36,042:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:112) finished in 1.752 s
[INFO ]20161116@12:37:36,043:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 434 ms on localhost (1/1)
[INFO ]20161116@12:37:36,044:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:37:36,048:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@12:37:36,048:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161116@12:37:36,063:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@12:37:36,063:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@12:37:36,078:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:113) finished in 1.648 s
[INFO ]20161116@12:37:36,148:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@12:37:36,148:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@12:37:36,148:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@12:37:36,148:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@12:37:36,152:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:115), which has no missing parents
[INFO ]20161116@12:37:36,396:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 322.4 KB)
[INFO ]20161116@12:37:36,404:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1914.0 B, free 324.2 KB)
[INFO ]20161116@12:37:36,430:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:57124 (size: 1914.0 B, free: 500.1 MB)
[INFO ]20161116@12:37:36,432:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:37:36,434:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:115)
[INFO ]20161116@12:37:36,434:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@12:37:36,437:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@12:37:36,442:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@12:37:36,491:org.apache.spark.CacheManager - Partition rdd_8_0 not found, computing it
[INFO ]20161116@12:37:36,512:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@12:37:36,535:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 25 ms
[INFO ]20161116@12:37:36,712:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@12:37:36,713:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161116@12:37:38,141:org.apache.spark.storage.MemoryStore - Block rdd_8_0 stored as values in memory (estimated size 198.3 KB, free 522.6 KB)
[INFO ]20161116@12:37:38,145:org.apache.spark.storage.BlockManagerInfo - Added rdd_8_0 in memory on localhost:57124 (size: 198.3 KB, free: 499.9 MB)
[INFO ]20161116@12:37:38,181:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1704 bytes result sent to driver
[INFO ]20161116@12:37:38,256:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 1820 ms on localhost (1/1)
[INFO ]20161116@12:37:38,256:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:37:38,323:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:121) finished in 1.875 s
[INFO ]20161116@12:37:38,760:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:121, took 6.571774 s
[INFO ]20161116@12:37:39,186:org.apache.spark.SparkContext - Starting job: collect at HW2_Part2.java:124
[INFO ]20161116@12:37:39,210:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161116@12:37:39,215:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161116@12:37:39,215:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part2.java:124) with 1 output partitions
[INFO ]20161116@12:37:39,216:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at HW2_Part2.java:124)
[INFO ]20161116@12:37:39,216:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@12:37:39,218:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@12:37:39,219:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:115), which has no missing parents
[INFO ]20161116@12:37:39,221:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 526.0 KB)
[INFO ]20161116@12:37:39,226:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1950.0 B, free 527.9 KB)
[INFO ]20161116@12:37:39,227:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:57124 (size: 1950.0 B, free: 499.9 MB)
[INFO ]20161116@12:37:39,228:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:37:39,228:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:115)
[INFO ]20161116@12:37:39,228:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@12:37:39,275:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@12:37:39,276:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161116@12:37:39,315:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@12:37:39,349:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 35565 bytes result sent to driver
[INFO ]20161116@12:37:39,537:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at HW2_Part2.java:124) finished in 0.305 s
[INFO ]20161116@12:37:39,539:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at HW2_Part2.java:124, took 0.352459 s
[INFO ]20161116@12:37:39,540:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 302 ms on localhost (1/1)
[INFO ]20161116@12:37:39,541:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:37:40,749:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@12:37:40,763:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:153
[INFO ]20161116@12:37:41,481:org.apache.spark.scheduler.DAGScheduler - Got job 2 (takeOrdered at HW2_Part2.java:153) with 1 output partitions
[INFO ]20161116@12:37:41,482:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (takeOrdered at HW2_Part2.java:153)
[INFO ]20161116@12:37:41,483:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161116@12:37:41,484:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@12:37:41,485:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:153), which has no missing parents
[INFO ]20161116@12:37:41,613:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.9 KB, free 532.7 KB)
[INFO ]20161116@12:37:41,624:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 535.4 KB)
[INFO ]20161116@12:37:41,762:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:57124 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@12:37:41,867:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:37:41,867:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:153)
[INFO ]20161116@12:37:41,870:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161116@12:37:41,897:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@12:37:41,900:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 4)
[INFO ]20161116@12:37:41,963:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@12:37:41,972:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:57124 in memory (size: 1950.0 B, free: 499.9 MB)
[INFO ]20161116@12:37:42,202:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 4). 2261 bytes result sent to driver
[INFO ]20161116@12:37:42,306:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (takeOrdered at HW2_Part2.java:153) finished in 0.434 s
[INFO ]20161116@12:37:42,307:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: takeOrdered at HW2_Part2.java:153, took 0.859059 s
[INFO ]20161116@12:37:42,313:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 4) in 414 ms on localhost (1/1)
[INFO ]20161116@12:37:42,317:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:37:42,436:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:184
[INFO ]20161116@12:37:42,441:org.apache.spark.scheduler.DAGScheduler - Got job 3 (takeOrdered at HW2_Part2.java:184) with 1 output partitions
[INFO ]20161116@12:37:42,443:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (takeOrdered at HW2_Part2.java:184)
[INFO ]20161116@12:37:42,443:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9, ShuffleMapStage 10)
[INFO ]20161116@12:37:42,444:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@12:37:42,446:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:184), which has no missing parents
[INFO ]20161116@12:37:42,457:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 534.9 KB)
[INFO ]20161116@12:37:42,504:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 537.6 KB)
[INFO ]20161116@12:37:42,505:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:57124 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@12:37:42,506:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:37:42,512:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:184)
[INFO ]20161116@12:37:42,512:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
[INFO ]20161116@12:37:42,514:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@12:37:42,515:org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 5)
[INFO ]20161116@12:37:42,523:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@12:39:43,025:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@12:39:43,123:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:57124 in memory (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@12:40:53,042:org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 5). 2259 bytes result sent to driver
[INFO ]20161116@12:40:53,208:org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (takeOrdered at HW2_Part2.java:184) finished in 190.684 s
[INFO ]20161116@12:40:53,209:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: takeOrdered at HW2_Part2.java:184, took 190.769148 s
[INFO ]20161116@12:40:53,219:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 5) in 190694 ms on localhost (1/1)
[INFO ]20161116@12:40:53,219:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:40:53,259:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@12:40:53,409:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@12:40:53,497:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@12:40:53,591:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@12:40:53,594:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@12:40:53,596:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@12:40:53,599:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@12:40:53,613:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@12:40:53,614:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@12:40:53,615:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-804e7e34-09f7-4dfb-ace1-cedd76f5f8c3
[INFO ]20161116@12:44:39,018:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@12:44:39,606:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@12:44:40,133:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@12:44:40,134:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@12:44:40,135:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@12:44:40,607:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 45245.
[INFO ]20161116@12:44:41,240:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@12:44:41,324:Remoting - Starting remoting
[INFO ]20161116@12:44:41,866:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:42652]
[INFO ]20161116@12:44:41,871:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:42652]
[INFO ]20161116@12:44:41,905:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 42652.
[INFO ]20161116@12:44:41,960:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@12:44:42,004:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@12:44:42,040:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-8b23ba98-0798-4b18-954b-d0627135ce6d
[INFO ]20161116@12:44:42,089:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161116@12:44:42,239:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@12:44:43,048:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@12:44:43,062:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@12:44:43,359:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@12:44:43,446:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55794.
[INFO ]20161116@12:44:43,453:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 55794
[INFO ]20161116@12:44:43,455:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@12:44:43,463:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:55794 with 500.1 MB RAM, BlockManagerId(driver, localhost, 55794)
[INFO ]20161116@12:44:43,469:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@12:44:45,661:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@12:44:46,270:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@12:44:46,384:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:55794 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@12:44:46,562:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:110
[INFO ]20161116@12:44:46,906:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@12:44:46,996:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@12:44:46,997:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:55794 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@12:44:47,003:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:111
[INFO ]20161116@12:44:47,866:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@12:44:48,393:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@12:44:48,561:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:121
[INFO ]20161116@12:44:48,638:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:112)
[INFO ]20161116@12:44:48,640:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:113)
[INFO ]20161116@12:44:48,642:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:121) with 1 output partitions
[INFO ]20161116@12:44:48,643:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:121)
[INFO ]20161116@12:44:48,644:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@12:44:48,653:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@12:44:48,722:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:112), which has no missing parents
[INFO ]20161116@12:44:49,099:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161116@12:44:49,228:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161116@12:44:49,228:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:55794 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@12:44:49,229:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:44:49,236:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:112)
[INFO ]20161116@12:44:49,238:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@12:44:49,255:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:113), which has no missing parents
[INFO ]20161116@12:44:49,260:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 316.8 KB)
[INFO ]20161116@12:44:49,272:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161116@12:44:49,431:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161116@12:44:49,440:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:55794 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@12:44:49,441:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:44:49,441:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:113)
[INFO ]20161116@12:44:49,456:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@12:44:49,448:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@12:44:49,851:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@12:44:50,005:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@12:44:50,007:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@12:44:50,008:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@12:44:50,008:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@12:44:50,008:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@12:44:50,698:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161116@12:44:50,727:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2173 bytes)
[INFO ]20161116@12:44:50,728:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@12:44:50,737:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@12:44:51,220:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161116@12:44:51,240:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1752 ms on localhost (1/1)
[INFO ]20161116@12:44:51,247:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:112) finished in 1.985 s
[INFO ]20161116@12:44:51,249:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@12:44:51,249:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161116@12:44:51,245:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:44:51,256:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@12:44:51,256:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@12:44:51,667:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:113) finished in 2.209 s
[INFO ]20161116@12:44:51,667:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@12:44:51,667:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@12:44:51,667:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@12:44:51,667:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@12:44:51,669:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:115), which has no missing parents
[INFO ]20161116@12:44:51,675:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 942 ms on localhost (1/1)
[INFO ]20161116@12:44:51,676:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:44:51,705:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 322.4 KB)
[INFO ]20161116@12:44:51,768:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1912.0 B, free 324.2 KB)
[INFO ]20161116@12:44:51,769:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:55794 (size: 1912.0 B, free: 500.1 MB)
[INFO ]20161116@12:44:51,771:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:44:51,778:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:115)
[INFO ]20161116@12:44:51,780:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@12:44:51,783:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@12:44:51,786:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@12:44:51,904:org.apache.spark.CacheManager - Partition rdd_8_0 not found, computing it
[INFO ]20161116@12:44:51,930:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@12:44:51,934:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
[INFO ]20161116@12:44:52,206:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@12:44:52,207:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
[INFO ]20161116@12:44:53,183:org.apache.spark.storage.MemoryStore - Block rdd_8_0 stored as values in memory (estimated size 198.3 KB, free 522.6 KB)
[INFO ]20161116@12:44:53,183:org.apache.spark.storage.BlockManagerInfo - Added rdd_8_0 in memory on localhost:55794 (size: 198.3 KB, free: 499.9 MB)
[INFO ]20161116@12:44:53,198:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1704 bytes result sent to driver
[INFO ]20161116@12:44:53,254:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:121) finished in 1.465 s
[INFO ]20161116@12:44:53,255:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 1466 ms on localhost (1/1)
[INFO ]20161116@12:44:53,258:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:44:53,292:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:121, took 4.726630 s
[INFO ]20161116@12:44:53,345:org.apache.spark.SparkContext - Starting job: collect at HW2_Part2.java:124
[INFO ]20161116@12:44:53,366:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161116@12:44:53,375:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161116@12:44:53,375:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part2.java:124) with 1 output partitions
[INFO ]20161116@12:44:53,383:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at HW2_Part2.java:124)
[INFO ]20161116@12:44:53,384:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@12:44:53,429:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@12:44:53,431:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:115), which has no missing parents
[INFO ]20161116@12:44:53,438:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 526.0 KB)
[INFO ]20161116@12:44:53,621:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1948.0 B, free 527.9 KB)
[INFO ]20161116@12:44:53,622:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:55794 (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161116@12:44:53,627:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:44:53,629:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:115)
[INFO ]20161116@12:44:53,631:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@12:44:53,643:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@12:44:53,645:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161116@12:44:53,680:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@12:44:53,695:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 35565 bytes result sent to driver
[INFO ]20161116@12:44:53,754:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at HW2_Part2.java:124) finished in 0.114 s
[INFO ]20161116@12:44:53,756:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at HW2_Part2.java:124, took 0.402147 s
[INFO ]20161116@12:44:53,758:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 115 ms on localhost (1/1)
[INFO ]20161116@12:44:53,758:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:44:54,659:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@12:44:54,678:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:153
[INFO ]20161116@12:44:54,739:org.apache.spark.scheduler.DAGScheduler - Got job 2 (takeOrdered at HW2_Part2.java:153) with 1 output partitions
[INFO ]20161116@12:44:54,739:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (takeOrdered at HW2_Part2.java:153)
[INFO ]20161116@12:44:54,740:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161116@12:44:54,740:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@12:44:54,744:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:153), which has no missing parents
[INFO ]20161116@12:44:54,776:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.9 KB, free 532.7 KB)
[INFO ]20161116@12:44:54,782:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 535.4 KB)
[INFO ]20161116@12:44:54,783:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:55794 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@12:44:54,783:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:44:54,783:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:153)
[INFO ]20161116@12:44:54,784:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161116@12:44:54,784:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@12:44:54,785:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 4)
[INFO ]20161116@12:44:54,801:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:55794 in memory (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161116@12:44:54,840:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@12:44:54,864:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 4). 2261 bytes result sent to driver
[INFO ]20161116@12:44:54,882:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (takeOrdered at HW2_Part2.java:153) finished in 0.097 s
[INFO ]20161116@12:44:54,884:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: takeOrdered at HW2_Part2.java:153, took 0.204402 s
[INFO ]20161116@12:44:54,890:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 4) in 98 ms on localhost (1/1)
[INFO ]20161116@12:44:54,891:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:44:54,925:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:184
[INFO ]20161116@12:44:54,935:org.apache.spark.scheduler.DAGScheduler - Got job 3 (takeOrdered at HW2_Part2.java:184) with 1 output partitions
[INFO ]20161116@12:44:54,935:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (takeOrdered at HW2_Part2.java:184)
[INFO ]20161116@12:44:54,935:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9, ShuffleMapStage 10)
[INFO ]20161116@12:44:54,936:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@12:44:54,938:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:184), which has no missing parents
[INFO ]20161116@12:44:54,941:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 534.9 KB)
[INFO ]20161116@12:44:54,966:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 537.6 KB)
[INFO ]20161116@12:44:54,967:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:55794 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@12:44:54,971:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@12:44:54,973:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:184)
[INFO ]20161116@12:44:54,973:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
[INFO ]20161116@12:44:54,974:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@12:44:54,974:org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 5)
[INFO ]20161116@12:44:54,990:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@12:44:58,215:org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 5). 2259 bytes result sent to driver
[INFO ]20161116@12:44:58,288:org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (takeOrdered at HW2_Part2.java:184) finished in 3.304 s
[INFO ]20161116@12:44:58,289:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: takeOrdered at HW2_Part2.java:184, took 3.361178 s
[INFO ]20161116@12:44:58,290:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 5) in 3313 ms on localhost (1/1)
[INFO ]20161116@12:44:58,290:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ]20161116@12:44:58,347:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@12:44:58,399:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@12:44:58,402:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:55794 in memory (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@12:44:58,403:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161116@12:44:58,410:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:55794 in memory (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@12:44:58,463:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@12:44:58,546:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@12:44:58,636:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@12:44:58,637:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@12:44:58,724:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@12:44:58,797:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@12:44:58,877:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@12:44:58,885:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@12:44:58,892:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-59791940-9905-402f-90d3-0a58027d7b44
[INFO ]20161116@12:44:58,919:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@13:07:36,664:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@13:07:41,588:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@13:07:44,079:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@13:07:44,083:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@13:07:44,084:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@13:07:47,252:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 42379.
[INFO ]20161116@13:07:51,882:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@13:07:52,797:Remoting - Starting remoting
[INFO ]20161116@13:07:55,356:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:49991]
[INFO ]20161116@13:07:55,634:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:49991]
[INFO ]20161116@13:07:55,691:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 49991.
[INFO ]20161116@13:07:56,161:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@13:07:56,549:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@13:07:56,664:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-76db3af1-a501-44e0-b1f3-7e2a1a1d1f2a
[INFO ]20161116@13:07:57,030:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161116@13:07:58,081:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@13:08:01,374:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@13:08:01,404:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@13:08:02,244:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@13:08:02,436:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51305.
[INFO ]20161116@13:08:02,487:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 51305
[INFO ]20161116@13:08:02,497:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@13:08:02,696:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:51305 with 500.1 MB RAM, BlockManagerId(driver, localhost, 51305)
[INFO ]20161116@13:08:02,703:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@13:08:08,675:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@13:08:10,073:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@13:08:10,139:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:51305 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@13:08:10,306:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:111
[INFO ]20161116@13:08:10,783:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@13:08:10,969:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@13:08:11,025:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:51305 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@13:08:11,030:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:112
[INFO ]20161116@13:08:12,651:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@13:08:13,233:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@13:08:13,457:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:122
[INFO ]20161116@13:08:13,749:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:113)
[INFO ]20161116@13:08:13,753:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:114)
[INFO ]20161116@13:08:13,769:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:122) with 1 output partitions
[INFO ]20161116@13:08:13,782:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:122)
[INFO ]20161116@13:08:13,861:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@13:08:13,981:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@13:08:14,114:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:113), which has no missing parents
[INFO ]20161116@13:08:14,555:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161116@13:08:14,718:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161116@13:08:14,725:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:51305 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@13:08:14,728:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:08:14,748:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:113)
[INFO ]20161116@13:08:14,755:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@13:08:14,791:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:114), which has no missing parents
[INFO ]20161116@13:08:14,964:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 316.8 KB)
[INFO ]20161116@13:08:15,218:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161116@13:08:15,295:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161116@13:08:15,379:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:51305 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@13:08:15,395:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@13:08:15,429:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:08:15,429:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:114)
[INFO ]20161116@13:08:15,430:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@13:08:15,787:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@13:08:15,890:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@13:08:15,890:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@13:08:15,890:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@13:08:15,890:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@13:08:15,890:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@13:08:17,590:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161116@13:08:17,625:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2173 bytes)
[INFO ]20161116@13:08:17,628:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@13:08:17,643:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@13:08:18,496:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 3226 ms on localhost (1/1)
[INFO ]20161116@13:08:18,512:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:08:18,644:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:113) finished in 3.860 s
[INFO ]20161116@13:08:18,652:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@13:08:18,649:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161116@13:08:18,662:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161116@13:08:18,663:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@13:08:18,664:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@13:08:18,728:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:114) finished in 3.297 s
[INFO ]20161116@13:08:18,728:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@13:08:18,728:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@13:08:18,728:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@13:08:18,729:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@13:08:18,729:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:116), which has no missing parents
[INFO ]20161116@13:08:18,730:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1105 ms on localhost (1/1)
[INFO ]20161116@13:08:18,730:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:08:18,880:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 322.4 KB)
[INFO ]20161116@13:08:18,915:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1912.0 B, free 324.2 KB)
[INFO ]20161116@13:08:18,918:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:51305 (size: 1912.0 B, free: 500.1 MB)
[INFO ]20161116@13:08:19,018:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:08:19,053:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:116)
[INFO ]20161116@13:08:19,054:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@13:08:19,057:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:08:19,059:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@13:08:19,096:org.apache.spark.CacheManager - Partition rdd_8_0 not found, computing it
[INFO ]20161116@13:08:19,350:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@13:08:19,354:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
[INFO ]20161116@13:08:19,637:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@13:08:19,641:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
[INFO ]20161116@13:08:21,959:org.apache.spark.storage.MemoryStore - Block rdd_8_0 stored as values in memory (estimated size 198.3 KB, free 522.6 KB)
[INFO ]20161116@13:08:22,140:org.apache.spark.storage.BlockManagerInfo - Added rdd_8_0 in memory on localhost:51305 (size: 198.3 KB, free: 499.9 MB)
[INFO ]20161116@13:08:22,222:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1704 bytes result sent to driver
[INFO ]20161116@13:08:22,477:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 3421 ms on localhost (1/1)
[INFO ]20161116@13:08:22,478:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:08:22,637:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:122) finished in 3.579 s
[INFO ]20161116@13:08:22,760:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:122, took 9.302549 s
[INFO ]20161116@13:08:23,010:org.apache.spark.SparkContext - Starting job: collect at HW2_Part2.java:125
[INFO ]20161116@13:08:23,066:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161116@13:08:23,074:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161116@13:08:23,093:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part2.java:125) with 1 output partitions
[INFO ]20161116@13:08:23,094:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at HW2_Part2.java:125)
[INFO ]20161116@13:08:23,094:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@13:08:23,153:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@13:08:23,160:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:116), which has no missing parents
[INFO ]20161116@13:08:23,165:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 526.0 KB)
[INFO ]20161116@13:08:23,374:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1948.0 B, free 527.9 KB)
[INFO ]20161116@13:08:23,386:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:51305 (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161116@13:08:23,400:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:08:23,401:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:116)
[INFO ]20161116@13:08:23,401:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@13:08:23,458:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:08:23,458:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161116@13:08:23,581:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@13:08:23,644:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 35565 bytes result sent to driver
[INFO ]20161116@13:08:23,771:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at HW2_Part2.java:125) finished in 0.359 s
[INFO ]20161116@13:08:23,772:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at HW2_Part2.java:125, took 0.746145 s
[INFO ]20161116@13:08:23,779:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 356 ms on localhost (1/1)
[INFO ]20161116@13:08:23,780:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:08:24,423:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@13:08:24,491:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:156
[INFO ]20161116@13:08:24,539:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:51305 in memory (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161116@13:08:24,541:org.apache.spark.scheduler.DAGScheduler - Got job 2 (takeOrdered at HW2_Part2.java:156) with 1 output partitions
[INFO ]20161116@13:08:24,548:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (takeOrdered at HW2_Part2.java:156)
[INFO ]20161116@13:08:24,548:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161116@13:08:24,563:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@13:08:24,564:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:156), which has no missing parents
[INFO ]20161116@13:08:24,580:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.9 KB, free 527.4 KB)
[INFO ]20161116@13:08:24,732:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 530.1 KB)
[INFO ]20161116@13:08:24,735:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:51305 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@13:08:24,741:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:08:24,745:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:156)
[INFO ]20161116@13:08:24,745:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161116@13:08:24,749:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:08:24,749:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 4)
[INFO ]20161116@13:08:24,987:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@13:08:25,297:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 4). 2261 bytes result sent to driver
[INFO ]20161116@13:08:25,360:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 4) in 612 ms on localhost (1/1)
[INFO ]20161116@13:08:25,361:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:08:25,361:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (takeOrdered at HW2_Part2.java:156) finished in 0.610 s
[INFO ]20161116@13:08:25,362:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: takeOrdered at HW2_Part2.java:156, took 0.868446 s
[INFO ]20161116@13:08:25,654:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:187
[INFO ]20161116@13:08:25,657:org.apache.spark.scheduler.DAGScheduler - Got job 3 (takeOrdered at HW2_Part2.java:187) with 1 output partitions
[INFO ]20161116@13:08:25,657:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (takeOrdered at HW2_Part2.java:187)
[INFO ]20161116@13:08:25,657:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9, ShuffleMapStage 10)
[INFO ]20161116@13:08:25,659:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@13:08:25,660:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:187), which has no missing parents
[INFO ]20161116@13:08:25,663:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 534.9 KB)
[INFO ]20161116@13:08:25,687:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 537.6 KB)
[INFO ]20161116@13:08:25,704:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:51305 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@13:08:25,707:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:08:25,708:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:187)
[INFO ]20161116@13:08:25,708:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
[INFO ]20161116@13:08:25,711:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:08:25,711:org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 5)
[INFO ]20161116@13:08:25,738:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@13:08:50,552:org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 5). 2259 bytes result sent to driver
[INFO ]20161116@13:08:50,608:org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (takeOrdered at HW2_Part2.java:187) finished in 24.892 s
[INFO ]20161116@13:08:50,609:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: takeOrdered at HW2_Part2.java:187, took 24.953928 s
[INFO ]20161116@13:08:50,611:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 5) in 24895 ms on localhost (1/1)
[INFO ]20161116@13:08:50,628:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:08:50,700:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@13:08:50,702:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:51305 in memory (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@13:08:50,704:org.apache.spark.ContextCleaner - Cleaned accumulator 6
[INFO ]20161116@13:08:50,707:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:51305 in memory (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@13:08:50,729:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@13:08:50,838:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@13:08:50,882:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@13:08:50,949:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@13:08:50,954:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@13:08:50,959:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@13:08:50,970:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@13:08:50,987:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@13:08:50,988:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@13:08:50,989:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-35f1cc41-8d92-427e-a056-15335d8fcf3c
[INFO ]20161116@13:11:14,553:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@13:11:15,630:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@13:11:16,413:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@13:11:16,417:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@13:11:16,418:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@13:11:17,299:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 37207.
[INFO ]20161116@13:11:18,402:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@13:11:18,594:Remoting - Starting remoting
[INFO ]20161116@13:11:19,284:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:48507]
[INFO ]20161116@13:11:19,291:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:48507]
[INFO ]20161116@13:11:19,314:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 48507.
[INFO ]20161116@13:11:19,436:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@13:11:19,533:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@13:11:19,606:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-1d5838f5-aac3-40e0-a594-a091c2b3194e
[INFO ]20161116@13:11:19,692:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161116@13:11:19,913:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@13:11:21,297:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@13:11:21,302:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@13:11:23,409:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@13:11:24,522:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58697.
[INFO ]20161116@13:11:24,628:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 58697
[INFO ]20161116@13:11:24,634:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@13:11:24,710:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:58697 with 500.1 MB RAM, BlockManagerId(driver, localhost, 58697)
[INFO ]20161116@13:11:25,293:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@13:11:31,735:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@13:11:33,491:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@13:11:33,531:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:58697 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@13:11:33,753:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:111
[INFO ]20161116@13:11:34,250:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@13:11:34,443:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@13:11:34,447:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:58697 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@13:11:34,450:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:112
[INFO ]20161116@13:11:35,760:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@13:11:35,976:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@13:11:36,220:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:121
[INFO ]20161116@13:11:36,372:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:113)
[INFO ]20161116@13:11:36,373:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:114)
[INFO ]20161116@13:11:36,388:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:121) with 1 output partitions
[INFO ]20161116@13:11:36,392:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:121)
[INFO ]20161116@13:11:36,394:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@13:11:36,405:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@13:11:36,466:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:113), which has no missing parents
[INFO ]20161116@13:11:36,758:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161116@13:11:36,852:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161116@13:11:36,853:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:58697 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@13:11:36,854:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:11:36,871:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:113)
[INFO ]20161116@13:11:36,888:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@13:11:36,951:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:114), which has no missing parents
[INFO ]20161116@13:11:36,978:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 316.8 KB)
[INFO ]20161116@13:11:37,076:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161116@13:11:37,136:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161116@13:11:37,154:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:58697 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@13:11:37,156:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:11:37,156:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:114)
[INFO ]20161116@13:11:37,156:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@13:11:37,203:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@13:11:37,390:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@13:11:37,461:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@13:11:37,462:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@13:11:37,463:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@13:11:37,463:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@13:11:37,463:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@13:11:38,003:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161116@13:11:38,032:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2173 bytes)
[INFO ]20161116@13:11:38,035:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@13:11:38,072:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@13:11:38,572:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1316 ms on localhost (1/1)
[INFO ]20161116@13:11:38,597:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161116@13:11:38,602:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:113) finished in 1.653 s
[INFO ]20161116@13:11:38,585:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:11:38,619:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@13:11:38,619:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161116@13:11:38,620:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@13:11:38,621:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@13:11:38,717:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:114) finished in 1.561 s
[INFO ]20161116@13:11:38,717:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@13:11:38,717:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@13:11:38,717:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@13:11:38,717:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@13:11:38,720:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:116), which has no missing parents
[INFO ]20161116@13:11:38,724:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 676 ms on localhost (1/1)
[INFO ]20161116@13:11:38,724:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:11:38,830:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 322.4 KB)
[INFO ]20161116@13:11:38,940:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1912.0 B, free 324.2 KB)
[INFO ]20161116@13:11:38,959:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:58697 (size: 1912.0 B, free: 500.1 MB)
[INFO ]20161116@13:11:38,996:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:11:39,117:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:116)
[INFO ]20161116@13:11:39,117:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@13:11:39,123:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:11:39,126:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@13:11:39,179:org.apache.spark.CacheManager - Partition rdd_8_0 not found, computing it
[INFO ]20161116@13:11:39,228:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@13:11:39,310:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 89 ms
[INFO ]20161116@13:11:39,566:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@13:11:39,572:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
[INFO ]20161116@13:11:41,961:org.apache.spark.storage.MemoryStore - Block rdd_8_0 stored as values in memory (estimated size 198.3 KB, free 522.6 KB)
[INFO ]20161116@13:11:41,966:org.apache.spark.storage.BlockManagerInfo - Added rdd_8_0 in memory on localhost:58697 (size: 198.3 KB, free: 499.9 MB)
[INFO ]20161116@13:11:42,052:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1704 bytes result sent to driver
[INFO ]20161116@13:11:42,113:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 2979 ms on localhost (1/1)
[INFO ]20161116@13:11:42,116:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:11:42,243:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:121) finished in 3.122 s
[INFO ]20161116@13:11:42,278:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:121, took 6.056178 s
[INFO ]20161116@13:11:42,413:org.apache.spark.SparkContext - Starting job: collect at HW2_Part2.java:124
[INFO ]20161116@13:11:42,439:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161116@13:11:42,456:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161116@13:11:42,462:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part2.java:124) with 1 output partitions
[INFO ]20161116@13:11:42,462:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at HW2_Part2.java:124)
[INFO ]20161116@13:11:42,462:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@13:11:42,467:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@13:11:42,475:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:116), which has no missing parents
[INFO ]20161116@13:11:42,477:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 526.0 KB)
[INFO ]20161116@13:11:42,722:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1948.0 B, free 527.9 KB)
[INFO ]20161116@13:11:42,724:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:58697 (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161116@13:11:42,726:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:11:42,726:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:116)
[INFO ]20161116@13:11:42,726:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@13:11:42,944:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:11:42,945:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161116@13:11:42,974:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@13:11:42,992:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 35565 bytes result sent to driver
[INFO ]20161116@13:11:43,081:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at HW2_Part2.java:124) finished in 0.335 s
[INFO ]20161116@13:11:43,082:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at HW2_Part2.java:124, took 0.669152 s
[INFO ]20161116@13:11:43,084:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 333 ms on localhost (1/1)
[INFO ]20161116@13:11:43,086:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:11:44,108:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@13:11:44,165:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:157
[INFO ]20161116@13:11:44,222:org.apache.spark.scheduler.DAGScheduler - Got job 2 (takeOrdered at HW2_Part2.java:157) with 1 output partitions
[INFO ]20161116@13:11:44,223:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (takeOrdered at HW2_Part2.java:157)
[INFO ]20161116@13:11:44,226:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161116@13:11:44,576:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@13:11:44,576:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:157), which has no missing parents
[INFO ]20161116@13:11:44,654:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.9 KB, free 532.7 KB)
[INFO ]20161116@13:11:44,775:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:58697 in memory (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161116@13:11:44,805:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 530.1 KB)
[INFO ]20161116@13:11:44,808:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:58697 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@13:11:44,808:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:11:44,809:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:157)
[INFO ]20161116@13:11:45,235:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161116@13:11:45,318:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:11:45,318:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 4)
[INFO ]20161116@13:11:45,496:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@13:11:45,551:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 4). 2261 bytes result sent to driver
[INFO ]20161116@13:11:45,674:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (takeOrdered at HW2_Part2.java:157) finished in 0.436 s
[INFO ]20161116@13:11:45,676:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: takeOrdered at HW2_Part2.java:157, took 1.496567 s
[INFO ]20161116@13:11:45,677:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 4) in 435 ms on localhost (1/1)
[INFO ]20161116@13:11:45,677:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:11:46,618:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:189
[INFO ]20161116@13:11:46,845:org.apache.spark.scheduler.DAGScheduler - Got job 3 (takeOrdered at HW2_Part2.java:189) with 1 output partitions
[INFO ]20161116@13:11:46,923:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (takeOrdered at HW2_Part2.java:189)
[INFO ]20161116@13:11:46,923:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9, ShuffleMapStage 10)
[INFO ]20161116@13:11:46,925:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@13:11:46,928:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:189), which has no missing parents
[INFO ]20161116@13:11:46,930:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 534.9 KB)
[INFO ]20161116@13:11:46,989:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 537.6 KB)
[INFO ]20161116@13:11:46,991:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:58697 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@13:11:47,062:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:11:47,063:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:189)
[INFO ]20161116@13:11:47,063:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
[INFO ]20161116@13:11:47,065:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:11:47,066:org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 5)
[INFO ]20161116@13:11:47,319:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@13:12:20,006:org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 5). 2259 bytes result sent to driver
[INFO ]20161116@13:12:20,120:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@13:12:20,201:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:58697 in memory (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@13:12:20,241:org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (takeOrdered at HW2_Part2.java:189) finished in 33.175 s
[INFO ]20161116@13:12:20,243:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: takeOrdered at HW2_Part2.java:189, took 33.577831 s
[INFO ]20161116@13:12:20,251:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 5) in 33174 ms on localhost (1/1)
[INFO ]20161116@13:12:20,252:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:12:20,451:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@13:12:20,537:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@13:12:20,639:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@13:12:20,702:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@13:12:20,753:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@13:12:20,759:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@13:12:20,774:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@13:12:20,802:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
[INFO ]20161116@13:12:20,810:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@13:12:20,811:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@13:12:20,819:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-b05070f6-080c-49f6-89e4-4f0419c8f5e8
[INFO ]20161116@13:13:35,320:org.apache.spark.SparkContext - Running Spark version 1.6.0
[WARN ]20161116@13:13:36,273:org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO ]20161116@13:13:37,204:org.apache.spark.SecurityManager - Changing view acls to: cloudera
[INFO ]20161116@13:13:37,204:org.apache.spark.SecurityManager - Changing modify acls to: cloudera
[INFO ]20161116@13:13:37,205:org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)
[INFO ]20161116@13:13:38,081:org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39084.
[INFO ]20161116@13:13:39,131:akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[INFO ]20161116@13:13:39,281:Remoting - Starting remoting
[INFO ]20161116@13:13:39,847:Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:52990]
[INFO ]20161116@13:13:39,853:Remoting - Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.2.15:52990]
[INFO ]20161116@13:13:39,906:org.apache.spark.util.Utils - Successfully started service 'sparkDriverActorSystem' on port 52990.
[INFO ]20161116@13:13:40,014:org.apache.spark.SparkEnv - Registering MapOutputTracker
[INFO ]20161116@13:13:40,089:org.apache.spark.SparkEnv - Registering BlockManagerMaster
[INFO ]20161116@13:13:40,142:org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-90f6277b-7499-423d-b97d-cd007420911e
[INFO ]20161116@13:13:40,220:org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 500.1 MB
[INFO ]20161116@13:13:40,446:org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
[INFO ]20161116@13:13:42,364:org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
[INFO ]20161116@13:13:42,519:org.apache.spark.ui.SparkUI - Started SparkUI at http://10.0.2.15:4040
[INFO ]20161116@13:13:43,917:org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
[INFO ]20161116@13:13:44,222:org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37551.
[INFO ]20161116@13:13:44,235:org.apache.spark.network.netty.NettyBlockTransferService - Server created on 37551
[INFO ]20161116@13:13:44,243:org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
[INFO ]20161116@13:13:44,253:org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager localhost:37551 with 500.1 MB RAM, BlockManagerId(driver, localhost, 37551)
[INFO ]20161116@13:13:44,257:org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
[INFO ]20161116@13:13:48,624:org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 138.4 KB, free 138.4 KB)
[INFO ]20161116@13:13:49,785:org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 153.4 KB)
[INFO ]20161116@13:13:49,800:org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:37551 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@13:13:49,827:org.apache.spark.SparkContext - Created broadcast 0 from textFile at HW2_Part2.java:111
[INFO ]20161116@13:13:50,229:org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 138.5 KB, free 291.8 KB)
[INFO ]20161116@13:13:50,298:org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 306.8 KB)
[INFO ]20161116@13:13:50,301:org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:37551 (size: 14.9 KB, free: 500.1 MB)
[INFO ]20161116@13:13:50,303:org.apache.spark.SparkContext - Created broadcast 1 from textFile at HW2_Part2.java:112
[INFO ]20161116@13:13:51,794:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@13:13:53,331:org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
[INFO ]20161116@13:13:53,551:org.apache.spark.SparkContext - Starting job: count at HW2_Part2.java:121
[INFO ]20161116@13:13:53,646:org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (mapToPair at HW2_Part2.java:113)
[INFO ]20161116@13:13:53,653:org.apache.spark.scheduler.DAGScheduler - Registering RDD 5 (mapToPair at HW2_Part2.java:114)
[INFO ]20161116@13:13:53,662:org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at HW2_Part2.java:121) with 1 output partitions
[INFO ]20161116@13:13:53,662:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (count at HW2_Part2.java:121)
[INFO ]20161116@13:13:53,663:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@13:13:53,670:org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[INFO ]20161116@13:13:53,770:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:113), which has no missing parents
[INFO ]20161116@13:13:54,509:org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.9 KB, free 310.6 KB)
[INFO ]20161116@13:13:55,011:org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 312.9 KB)
[INFO ]20161116@13:13:55,011:org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:37551 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@13:13:55,013:org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:13:55,022:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at mapToPair at HW2_Part2.java:113)
[INFO ]20161116@13:13:55,030:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
[INFO ]20161116@13:13:55,149:org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:114), which has no missing parents
[INFO ]20161116@13:13:55,181:org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 3.9 KB, free 316.8 KB)
[INFO ]20161116@13:13:55,426:org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 319.1 KB)
[INFO ]20161116@13:13:55,438:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2185 bytes)
[INFO ]20161116@13:13:55,464:org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:37551 (size: 2.3 KB, free: 500.1 MB)
[INFO ]20161116@13:13:55,465:org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:13:55,465:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at mapToPair at HW2_Part2.java:114)
[INFO ]20161116@13:13:55,465:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
[INFO ]20161116@13:13:55,559:org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
[INFO ]20161116@13:13:55,983:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/SP500-constituents-financials.csv:0+82761
[INFO ]20161116@13:13:56,103:org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[INFO ]20161116@13:13:56,103:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[INFO ]20161116@13:13:56,103:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[INFO ]20161116@13:13:56,104:org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[INFO ]20161116@13:13:56,106:org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
[INFO ]20161116@13:13:57,161:org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2182 bytes result sent to driver
[INFO ]20161116@13:13:57,225:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2173 bytes)
[INFO ]20161116@13:13:57,227:org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
[INFO ]20161116@13:13:57,230:org.apache.spark.rdd.HadoopRDD - Input split: file:/home/cloudera/git/siming.meng/homework2/data/companies/companylistNASDAQ.csv:0+408959
[INFO ]20161116@13:13:57,792:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 2561 ms on localhost (1/1)
[INFO ]20161116@13:13:57,794:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:13:57,795:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at HW2_Part2.java:113) finished in 2.653 s
[INFO ]20161116@13:13:57,804:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@13:13:57,804:org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
[INFO ]20161116@13:13:57,804:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@13:13:57,805:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@13:13:57,863:org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2182 bytes result sent to driver
[INFO ]20161116@13:13:58,015:org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (mapToPair at HW2_Part2.java:114) finished in 2.550 s
[INFO ]20161116@13:13:58,015:org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
[INFO ]20161116@13:13:58,015:org.apache.spark.scheduler.DAGScheduler - running: Set()
[INFO ]20161116@13:13:58,015:org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
[INFO ]20161116@13:13:58,015:org.apache.spark.scheduler.DAGScheduler - failed: Set()
[INFO ]20161116@13:13:58,023:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:116), which has no missing parents
[INFO ]20161116@13:13:58,024:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 771 ms on localhost (1/1)
[INFO ]20161116@13:13:58,024:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:13:58,485:org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 322.4 KB)
[INFO ]20161116@13:13:58,690:org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1912.0 B, free 324.2 KB)
[INFO ]20161116@13:13:58,691:org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:37551 (size: 1912.0 B, free: 500.1 MB)
[INFO ]20161116@13:13:58,693:org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:13:58,705:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at join at HW2_Part2.java:116)
[INFO ]20161116@13:13:58,706:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
[INFO ]20161116@13:13:58,776:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:13:58,781:org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
[INFO ]20161116@13:13:58,870:org.apache.spark.CacheManager - Partition rdd_8_0 not found, computing it
[INFO ]20161116@13:13:59,136:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@13:13:59,351:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 246 ms
[INFO ]20161116@13:13:59,463:org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
[INFO ]20161116@13:13:59,474:org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
[INFO ]20161116@13:14:01,983:org.apache.spark.storage.MemoryStore - Block rdd_8_0 stored as values in memory (estimated size 198.3 KB, free 522.6 KB)
[INFO ]20161116@13:14:01,984:org.apache.spark.storage.BlockManagerInfo - Added rdd_8_0 in memory on localhost:37551 (size: 198.3 KB, free: 499.9 MB)
[INFO ]20161116@13:14:02,033:org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1704 bytes result sent to driver
[INFO ]20161116@13:14:02,064:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 3352 ms on localhost (1/1)
[INFO ]20161116@13:14:02,064:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:14:02,084:org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (count at HW2_Part2.java:121) finished in 3.375 s
[INFO ]20161116@13:14:02,131:org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at HW2_Part2.java:121, took 8.579068 s
[INFO ]20161116@13:14:02,274:org.apache.spark.SparkContext - Starting job: collect at HW2_Part2.java:124
[INFO ]20161116@13:14:02,464:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 0 is 143 bytes
[INFO ]20161116@13:14:02,487:org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 143 bytes
[INFO ]20161116@13:14:02,489:org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at HW2_Part2.java:124) with 1 output partitions
[INFO ]20161116@13:14:02,489:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at HW2_Part2.java:124)
[INFO ]20161116@13:14:02,491:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
[INFO ]20161116@13:14:02,536:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@13:14:02,543:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:116), which has no missing parents
[INFO ]20161116@13:14:02,545:org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.4 KB, free 526.0 KB)
[INFO ]20161116@13:14:02,613:org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1948.0 B, free 527.9 KB)
[INFO ]20161116@13:14:02,614:org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:37551 (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161116@13:14:02,615:org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:14:02,618:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at join at HW2_Part2.java:116)
[INFO ]20161116@13:14:02,620:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
[INFO ]20161116@13:14:02,660:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:14:02,663:org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 3)
[INFO ]20161116@13:14:02,762:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@13:14:02,768:org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 3). 35565 bytes result sent to driver
[INFO ]20161116@13:14:02,892:org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at HW2_Part2.java:124) finished in 0.216 s
[INFO ]20161116@13:14:02,893:org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at HW2_Part2.java:124, took 0.619075 s
[INFO ]20161116@13:14:02,903:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 3) in 233 ms on localhost (1/1)
[INFO ]20161116@13:14:02,904:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:14:03,145:org.apache.spark.ContextCleaner - Cleaned accumulator 4
[INFO ]20161116@13:14:03,620:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:37551 in memory (size: 1948.0 B, free: 499.9 MB)
[INFO ]20161116@13:14:03,887:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:157
[INFO ]20161116@13:14:03,902:org.apache.spark.scheduler.DAGScheduler - Got job 2 (takeOrdered at HW2_Part2.java:157) with 1 output partitions
[INFO ]20161116@13:14:03,904:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (takeOrdered at HW2_Part2.java:157)
[INFO ]20161116@13:14:03,904:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)
[INFO ]20161116@13:14:03,905:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@13:14:03,908:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:157), which has no missing parents
[INFO ]20161116@13:14:04,001:org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 4.9 KB, free 527.4 KB)
[INFO ]20161116@13:14:04,205:org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 530.1 KB)
[INFO ]20161116@13:14:04,208:org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:37551 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@13:14:04,213:org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:14:04,213:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[10] at takeOrdered at HW2_Part2.java:157)
[INFO ]20161116@13:14:04,213:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
[INFO ]20161116@13:14:04,214:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:14:04,215:org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 4)
[INFO ]20161116@13:14:04,260:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@13:14:04,366:org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 4). 2261 bytes result sent to driver
[INFO ]20161116@13:14:04,506:org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (takeOrdered at HW2_Part2.java:157) finished in 0.274 s
[INFO ]20161116@13:14:04,508:org.apache.spark.scheduler.DAGScheduler - Job 2 finished: takeOrdered at HW2_Part2.java:157, took 0.609589 s
[INFO ]20161116@13:14:04,511:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 4) in 292 ms on localhost (1/1)
[INFO ]20161116@13:14:04,512:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:14:04,871:org.apache.spark.SparkContext - Starting job: takeOrdered at HW2_Part2.java:189
[INFO ]20161116@13:14:04,892:org.apache.spark.scheduler.DAGScheduler - Got job 3 (takeOrdered at HW2_Part2.java:189) with 1 output partitions
[INFO ]20161116@13:14:04,893:org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (takeOrdered at HW2_Part2.java:189)
[INFO ]20161116@13:14:04,894:org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9, ShuffleMapStage 10)
[INFO ]20161116@13:14:04,896:org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
[INFO ]20161116@13:14:04,902:org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:189), which has no missing parents
[INFO ]20161116@13:14:04,915:org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 4.9 KB, free 534.9 KB)
[INFO ]20161116@13:14:04,982:org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 537.6 KB)
[INFO ]20161116@13:14:04,983:org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:37551 (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@13:14:04,983:org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
[INFO ]20161116@13:14:04,992:org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[12] at takeOrdered at HW2_Part2.java:189)
[INFO ]20161116@13:14:04,992:org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
[INFO ]20161116@13:14:04,993:org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 1967 bytes)
[INFO ]20161116@13:14:04,993:org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 5)
[INFO ]20161116@13:14:04,997:org.apache.spark.storage.BlockManager - Found block rdd_8_0 locally
[INFO ]20161116@13:14:06,647:org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 5). 2259 bytes result sent to driver
[INFO ]20161116@13:14:07,325:org.apache.spark.ContextCleaner - Cleaned accumulator 5
[INFO ]20161116@13:14:07,372:org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:37551 in memory (size: 2.6 KB, free: 499.9 MB)
[INFO ]20161116@13:14:07,442:org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (takeOrdered at HW2_Part2.java:189) finished in 2.444 s
[INFO ]20161116@13:14:07,443:org.apache.spark.scheduler.DAGScheduler - Job 3 finished: takeOrdered at HW2_Part2.java:189, took 2.571063 s
[INFO ]20161116@13:14:07,444:org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 5) in 2450 ms on localhost (1/1)
[INFO ]20161116@13:14:07,444:org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO ]20161116@13:14:07,701:org.apache.spark.SparkContext - Invoking stop() from shutdown hook
[INFO ]20161116@13:14:07,811:org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
[INFO ]20161116@13:14:07,896:org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
[INFO ]20161116@13:14:08,146:org.apache.spark.storage.MemoryStore - MemoryStore cleared
[INFO ]20161116@13:14:08,150:org.apache.spark.storage.BlockManager - BlockManager stopped
[INFO ]20161116@13:14:08,154:org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
[INFO ]20161116@13:14:08,166:org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
[INFO ]20161116@13:14:08,239:org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO ]20161116@13:14:08,239:org.apache.spark.util.ShutdownHookManager - Shutdown hook called
[INFO ]20161116@13:14:08,241:org.apache.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-8436e4cd-2b7d-40ff-89d0-9be365529bba
[INFO ]20161116@13:14:08,246:akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
